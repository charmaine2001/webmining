{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHt8C52GG-BL",
        "outputId": "1ad57c92-4e0c-4613-8f33-21291e4aace6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scrapy\n",
            "  Downloading scrapy-2.13.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (43.0.3)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.7.1)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.11.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.10.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.4.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pydispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: pyopenssl>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.1)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.8.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting twisted>=21.7.0 (from scrapy)\n",
            "  Downloading twisted-24.11.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting zope-interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (25.3.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.2)\n",
            "Collecting automat>=24.8.0 (from twisted>=21.7.0->scrapy)\n",
            "  Downloading automat-25.4.16-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting constantly>=15.1 (from twisted>=21.7.0->scrapy)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting hyperlink>=17.1.1 (from twisted>=21.7.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting incremental>=24.7.0 (from twisted>=21.7.0->scrapy)\n",
            "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from twisted>=21.7.0->scrapy) (4.13.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope-interface>=5.1.0->scrapy) (75.2.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (2.32.3)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.18.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2025.4.26)\n",
            "Downloading scrapy-2.13.0-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading itemadapter-0.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\n",
            "Downloading parsel-1.10.0-py2.py3-none-any.whl (17 kB)\n",
            "Downloading Protego-0.4.0-py2.py3-none-any.whl (8.6 kB)\n",
            "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Downloading queuelib-1.8.0-py3-none-any.whl (13 kB)\n",
            "Downloading service_identity-24.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading twisted-24.11.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading automat-25.4.16-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: pydispatcher, zope-interface, w3lib, queuelib, protego, jmespath, itemadapter, incremental, hyperlink, cssselect, constantly, automat, twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed automat-25.4.16 constantly-23.10.4 cssselect-1.3.0 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.11.0 itemloaders-1.3.2 jmespath-1.0.1 parsel-1.10.0 protego-0.4.0 pydispatcher-2.0.7 queuelib-1.8.0 requests-file-2.1.0 scrapy-2.13.0 service-identity-24.2.0 tldextract-5.3.0 twisted-24.11.0 w3lib-2.3.1 zope-interface-7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install scrapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qOcUIWw9HCCm",
        "outputId": "45251f3e-d6a5-40e4-9eaa-4e35f316abb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:scrapy.utils.log:Scrapy 2.13.0 started (bot: scrapybot)\n",
            "2025-05-12 13:06:22 [scrapy.utils.log] INFO: Scrapy 2.13.0 started (bot: scrapybot)\n",
            "INFO:scrapy.utils.log:Versions:\n",
            "{'lxml': '5.4.0',\n",
            " 'libxml2': '2.13.8',\n",
            " 'cssselect': '1.3.0',\n",
            " 'parsel': '1.10.0',\n",
            " 'w3lib': '2.3.1',\n",
            " 'Twisted': '24.11.0',\n",
            " 'Python': '3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]',\n",
            " 'pyOpenSSL': '24.2.1 (OpenSSL 3.3.2 3 Sep 2024)',\n",
            " 'cryptography': '43.0.3',\n",
            " 'Platform': 'Linux-6.1.123+-x86_64-with-glibc2.35'}\n",
            "2025-05-12 13:06:22 [scrapy.utils.log] INFO: Versions:\n",
            "{'lxml': '5.4.0',\n",
            " 'libxml2': '2.13.8',\n",
            " 'cssselect': '1.3.0',\n",
            " 'parsel': '1.10.0',\n",
            " 'w3lib': '2.3.1',\n",
            " 'Twisted': '24.11.0',\n",
            " 'Python': '3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]',\n",
            " 'pyOpenSSL': '24.2.1 (OpenSSL 3.3.2 3 Sep 2024)',\n",
            " 'cryptography': '43.0.3',\n",
            " 'Platform': 'Linux-6.1.123+-x86_64-with-glibc2.35'}\n",
            "INFO:scrapy.addons:Enabled addons:\n",
            "[]\n",
            "2025-05-12 13:06:22 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "DEBUG:scrapy.utils.log:Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-05-12 13:06:22 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "DEBUG:scrapy.utils.log:Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-05-12 13:06:22 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "INFO:scrapy.extensions.telnet:Telnet Password: 93f527ec758eaf8e\n",
            "2025-05-12 13:06:22 [scrapy.extensions.telnet] INFO: Telnet Password: 93f527ec758eaf8e\n",
            "/usr/local/lib/python3.11/dist-packages/scrapy/extensions/feedexport.py:455: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
            "  exporter = cls(crawler)\n",
            "INFO:scrapy.middleware:Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2025-05-12 13:06:23 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "INFO:scrapy.crawler:Overridden settings:\n",
            "{'CONCURRENT_REQUESTS_PER_DOMAIN': 2,\n",
            " 'DEPTH_LIMIT': 1,\n",
            " 'DOWNLOAD_DELAY': 1,\n",
            " 'FEED_EXPORT_FIELDS': ['title',\n",
            "                        'url',\n",
            "                        'section_name',\n",
            "                        'category',\n",
            "                        'source',\n",
            "                        'timestamp',\n",
            "                        'section_url'],\n",
            " 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
            "               '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
            "2025-05-12 13:06:23 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'CONCURRENT_REQUESTS_PER_DOMAIN': 2,\n",
            " 'DEPTH_LIMIT': 1,\n",
            " 'DOWNLOAD_DELAY': 1,\n",
            " 'FEED_EXPORT_FIELDS': ['title',\n",
            "                        'url',\n",
            "                        'section_name',\n",
            "                        'category',\n",
            "                        'source',\n",
            "                        'timestamp',\n",
            "                        'section_url'],\n",
            " 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
            "               '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
            "INFO:scrapy.middleware:Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2025-05-12 13:06:23 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "INFO:scrapy.middleware:Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n",
            " 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2025-05-12 13:06:23 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n",
            " 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "INFO:scrapy.middleware:Enabled item pipelines:\n",
            "[]\n",
            "2025-05-12 13:06:23 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "This event loop is already running",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a37586349264>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawlerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNewsSpider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scrapy/crawler.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[1;32m    500\u001b[0m                 \u001b[0;34m\"after\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"startup\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstall_shutdown_handlers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signal_shutdown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             )\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstall_signal_handlers\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_graceful_stop_reactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDeferred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/twisted/internet/asyncioreactor.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asyncioEventloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_forever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_justStopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_justStopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_forever\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;34m\"\"\"Run until stop() is called.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_coroutine_origin_tracking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_debug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/base_events.py\u001b[0m in \u001b[0;36m_check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This event loop is already running'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "class NewsSpider(scrapy.Spider):\n",
        "    name = 'enhanced_news_spider'\n",
        "    custom_settings = {\n",
        "        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'DOWNLOAD_DELAY': 1,\n",
        "        'CONCURRENT_REQUESTS_PER_DOMAIN': 2,\n",
        "        'FEED_FORMAT': 'csv',\n",
        "        'FEED_URI': 'enhanced_news_articles.csv',\n",
        "        'DEPTH_LIMIT': 1,\n",
        "        'FEED_EXPORT_FIELDS': ['title', 'url', 'section_name', 'category', 'source', 'timestamp', 'section_url']\n",
        "    }\n",
        "\n",
        "    SECTION_MAPPING = {\n",
        "        'business': ['business', 'economy', 'finance', 'markets', 'money', 'invest', 'stocks', 'companies', 'corporate', 'trade'], # <-- ADDED BUSINESS\n",
        "        'politics': ['politics', 'government', 'election', 'parliament', 'congress', 'democracy'],\n",
        "        'sports': ['sports', 'football', 'soccer', 'basketball', 'tennis', 'golf', 'olympics'],\n",
        "        'culture': ['culture', 'arts', 'entertainment', 'movies', 'music', 'television', 'theatre', 'gallery', 'celebrities']\n",
        "    }\n",
        "\n",
        "    # Alternative channels for each category if primary doesn't have it\n",
        "    ALTERNATIVE_CHANNELS = {\n",
        "        'business': ['https://www.bbc.com/news/business', 'https://edition.cnn.com/business', 'https://www.aljazeera.com/economy/'], # <-- ADDED BUSINESS\n",
        "        'politics': ['https://www.aljazeera.com', 'https://www.theguardian.com/international'],\n",
        "        'sports': ['https://www.bbc.com/sport', 'https://edition.cnn.com/sport'],\n",
        "        'culture': ['https://www.bbc.com/culture', 'https://www.theguardian.com/uk/culture']\n",
        "    }\n",
        "\n",
        "    start_urls = [\n",
        "        'https://www.bbc.com',\n",
        "        'https://edition.cnn.com',\n",
        "        'https://www.aljazeera.com',\n",
        "        'https://www.theguardian.com/international',\n",
        "        'https://www.newsday.co.zw'\n",
        "    ]\n",
        "\n",
        "    def parse(self, response):\n",
        "        if response.meta.get('is_section_page'):\n",
        "            self.logger.info(f\"Extracting articles from section page: {response.url}\")\n",
        "            yield from self.extract_articles(response)\n",
        "        elif response.url in self.start_urls:\n",
        "            self.logger.info(f\"Discovering sections from homepage: {response.url}\")\n",
        "            yield from self.discover_sections(response)\n",
        "        else:\n",
        "            self.logger.warning(f\"Unexpected URL in parse(): {response.url}\")\n",
        "            if 'source' not in response.meta:\n",
        "                    response.meta['source'] = self.get_source_name(response.url)\n",
        "            if 'category' not in response.meta:\n",
        "                    response.meta['category'] = self.detect_category(response)\n",
        "            if 'potential_section_name' not in response.meta:\n",
        "                    response.meta['potential_section_name'] = ''\n",
        "            yield from self.extract_articles(response)\n",
        "\n",
        "    def discover_sections(self, response):\n",
        "        \"\"\"Discover relevant sections from homepage navigation\"\"\"\n",
        "        link_extractor = LinkExtractor(\n",
        "            restrict_css='nav, ul.menu, div.navigation, header nav, footer nav, .primary-nav',\n",
        "            deny_extensions=['jpg', 'png', 'pdf']\n",
        "        )\n",
        "\n",
        "        found_categories = set()\n",
        "\n",
        "        for link in link_extractor.extract_links(response):\n",
        "            url = link.url.lower()\n",
        "            text = link.text.lower() if link.text else ''\n",
        "\n",
        "            for category, keywords in self.SECTION_MAPPING.items():\n",
        "                if any(kw in url or kw in text for kw in keywords):\n",
        "                    found_categories.add(category)\n",
        "                    meta_data = {\n",
        "                        'category': category,\n",
        "                        'source': self.get_source_name(response.url),\n",
        "                        'potential_section_name': link.text.strip() if link.text else '',\n",
        "                        'is_section_page': True\n",
        "                    }\n",
        "                    yield scrapy.Request(\n",
        "                        url=link.url,\n",
        "                        callback=self.parse,\n",
        "                        meta=meta_data\n",
        "                    )\n",
        "                    break\n",
        "\n",
        "        # Check for missing categories and suggest alternatives\n",
        "        # This logic now applies to the current source (response.url)\n",
        "        current_source_domain = self.get_source_name(response.url).lower().replace(' ', '') # e.g. \"bbc\" from \"BBC\"\n",
        "\n",
        "        missing_categories_on_this_source = set(self.SECTION_MAPPING.keys()) - found_categories\n",
        "        for category in missing_categories_on_this_source:\n",
        "            self.logger.info(f\"No '{category}' section found on {response.url} via navigation links. Checking ALTERNATIVE_CHANNELS.\")\n",
        "            for alt_url in self.ALTERNATIVE_CHANNELS.get(category, []):\n",
        "                # Only yield alternative if it belongs to the current source or is a general alternative not tied to a source\n",
        "                alt_source_domain = self.get_source_name(alt_url).lower().replace(' ', '')\n",
        "                # Check if the alternative URL's domain matches the current response's domain\n",
        "                # This ensures we only try to find e.g. BBC's business section when on BBC.\n",
        "                if any(s in alt_url for s in self.start_urls if current_source_domain in s) or \\\n",
        "                   any(s_domain in alt_url for s_domain in [d.split('//')[-1].split('/')[0] for d in self.start_urls if current_source_domain in d.split('//')[-1].split('/')[0]]):\n",
        "\n",
        "                    self.logger.info(f\"Attempting to use alternative URL for '{category}' on {current_source_domain}: {alt_url}\")\n",
        "                    yield scrapy.Request(\n",
        "                        url=alt_url,\n",
        "                        callback=self.parse, # Send to parse, it will then go to extract_articles\n",
        "                        meta={\n",
        "                            'category': category, # Pre-assign category\n",
        "                            'source': self.get_source_name(alt_url), # Source from the alt_url itself\n",
        "                            'potential_section_name': category.capitalize(), # Use category as fallback section name\n",
        "                            'is_section_page': True # Mark as section page\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "\n",
        "    def extract_articles(self, response):\n",
        "        \"\"\"Extract articles from section pages with proper section names\"\"\"\n",
        "        source = response.meta.get('source', self.get_source_name(response.url))\n",
        "        category = response.meta.get('category', self.detect_category(response))\n",
        "        potential_section_name = response.meta.get('potential_section_name', '')\n",
        "\n",
        "        section_name = self.extract_section_name(response, potential_section_name)\n",
        "\n",
        "        article_selectors = {\n",
        "            'bbc.com': 'a[href*=\"/news/\"], a[href*=\"/sport/\"], a[href*=\"/culture/\"], a[href*=\"/business/\"]', # Added /business/ for BBC\n",
        "            'cnn.com': 'a[href*=\"/article/\"], a[href*=\"/202\"], .container__link, a[href*=\"/business/\"]', # Added /business/ for CNN\n",
        "            'aljazeera.com': 'a.u-clickable-card__link, a.gc__title, h3 a, a[href*=\"/economy/\"]', # Added /economy/ for Al Jazeera\n",
        "            'theguardian.com': 'a[data-link-name=\"article\"], .fc-item__link, a[href*=\"/business/\"]', # Added /business/ for Guardian\n",
        "            'newsday.co.zw': 'a.story-link, h2 a' # NewsDay might need a specific business selector if available\n",
        "        }\n",
        "\n",
        "        selector_str = None\n",
        "        for domain, sel in article_selectors.items():\n",
        "            if domain in response.url:\n",
        "                selector_str = sel\n",
        "                break\n",
        "        if not selector_str:\n",
        "            selector_str = 'a[href*=\"/article/\"], a[href*=\"/news/\"], a[href*=\"/story/\"], a[href*=\"/post/\"], a[href*=\"/business/\"]' # Added generic business\n",
        "\n",
        "        articles = response.css(selector_str)\n",
        "\n",
        "        article_count = 0\n",
        "        for article in articles:\n",
        "            if article_count >= 10:\n",
        "                break\n",
        "\n",
        "            title_parts = article.css('::text').getall()\n",
        "            title = \" \".join(part.strip() for part in title_parts if part.strip()).strip()\n",
        "            title = re.sub(r'\\s+', ' ', title)\n",
        "\n",
        "            href = article.css('::attr(href)').get()\n",
        "\n",
        "            if not title or not href:\n",
        "                continue\n",
        "\n",
        "            if len(title) < 15 or len(title) > 200:\n",
        "                self.logger.debug(f\"Skipping article with title length issue: '{title}' from {response.url}\")\n",
        "                continue\n",
        "\n",
        "            url = response.urljoin(href)\n",
        "\n",
        "            yield {\n",
        "                'title': title,\n",
        "                'url': url,\n",
        "                'section_name': section_name,\n",
        "                'category': category,\n",
        "                'source': source,\n",
        "                'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                'section_url': response.url\n",
        "            }\n",
        "            article_count += 1\n",
        "        if article_count == 0:\n",
        "            self.logger.info(f\"No articles found on {response.url} with selector '{selector_str}'\")\n",
        "\n",
        "    def extract_section_name(self, response, fallback_name):\n",
        "        \"\"\"Extract the actual section name from the page\"\"\"\n",
        "        selectors = [\n",
        "            'h1.section-title::text', '.section-header h1::text', '.page-title::text',\n",
        "            'h1.title::text', 'h1.headline::text', '.content__label::text',\n",
        "            'header h1::text',\n",
        "            'h1::text'\n",
        "        ]\n",
        "        section_name_str = ''\n",
        "        for sel in selectors:\n",
        "            section_name_str = response.css(sel).get('')\n",
        "            if section_name_str.strip():\n",
        "                break\n",
        "\n",
        "        if not section_name_str.strip():\n",
        "            section_name_str = fallback_name\n",
        "\n",
        "        section_name_str = re.sub(r'[^\\w\\s-]', '', section_name_str).strip()\n",
        "\n",
        "        if not section_name_str:\n",
        "            self.logger.debug(f\"Section name not found via selectors/fallback for {response.url}. Trying URL parsing.\")\n",
        "            path_parts = [part for part in response.url.split('/') if part]\n",
        "            generic_terms = {'www', 'com', 'co', 'uk', 'org', 'news', 'article', 'category'}\n",
        "            for part in reversed(path_parts[-3:]):\n",
        "                if part and len(part) > 3 and part.lower() not in generic_terms and not part.isdigit():\n",
        "                    section_name_str = part.replace('-', ' ').replace('_', ' ').title()\n",
        "                    break\n",
        "\n",
        "        return section_name_str[:100]\n",
        "\n",
        "    def get_source_name(self, url):\n",
        "        \"\"\"Extract source name from URL\"\"\"\n",
        "        domain_map = {\n",
        "            'bbc.com': 'BBC',\n",
        "            'cnn.com': 'CNN',\n",
        "            'aljazeera.com': 'Al Jazeera',\n",
        "            'theguardian.com': 'The Guardian',\n",
        "            'newsday.co.zw': 'NewsDay Zimbabwe'\n",
        "        }\n",
        "        for domain, name in domain_map.items():\n",
        "            if domain in url:\n",
        "                return name\n",
        "        try:\n",
        "            return url.split('//')[-1].split('/')[0].replace('www.', '')\n",
        "        except IndexError:\n",
        "            return 'Unknown Source'\n",
        "\n",
        "    def detect_category(self, response):\n",
        "        \"\"\"Detect category from multiple signals\"\"\"\n",
        "        url_lower = response.url.lower()\n",
        "        title_lower = response.css('title::text').get('').lower()\n",
        "        meta_keywords_lower = response.css('meta[name=\"keywords\"]::attr(content)').get('').lower()\n",
        "\n",
        "        breadcrumb_texts = response.css('.breadcrumb a::text, .breadcrumbs a::text, [itemtype*=\"BreadcrumbList\"] span[itemprop=\"name\"]::text').getall()\n",
        "        breadcrumb_str = ' '.join(text.lower().strip() for text in breadcrumb_texts)\n",
        "\n",
        "        signals_text = [\n",
        "            url_lower,\n",
        "            title_lower,\n",
        "            meta_keywords_lower,\n",
        "            breadcrumb_str,\n",
        "            response.css('meta[property=\"og:section\"]::attr(content)').get('').lower(),\n",
        "            response.css('meta[name=\"section\"]::attr(content)').get('').lower(),\n",
        "            response.css('meta[name=\"parsely-section\"]::attr(content)').get('').lower()\n",
        "        ]\n",
        "\n",
        "        for text_signal in signals_text:\n",
        "            if not text_signal:\n",
        "                continue\n",
        "            for category, keywords in self.SECTION_MAPPING.items():\n",
        "                if any(kw in text_signal for kw in keywords):\n",
        "                    return category\n",
        "        return 'general'\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process = CrawlerProcess()\n",
        "    process.crawl(NewsSpider)\n",
        "    process.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUGPXQL_UflI",
        "outputId": "db314102-2035-4582-9f90-8b690e84cb4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.11/dist-packages (2.13.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (43.0.3)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.3.0)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.7.1)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.11.0)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.3.2)\n",
            "Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.10.0)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.4.0)\n",
            "Requirement already satisfied: pydispatcher>=2.0.5 in /usr/local/lib/python3.11/dist-packages (from scrapy) (2.0.7)\n",
            "Requirement already satisfied: pyopenssl>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.1)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.8.0)\n",
            "Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.0)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.3.0)\n",
            "Requirement already satisfied: twisted>=21.7.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.11.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (2.3.1)\n",
            "Requirement already satisfied: zope-interface>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (7.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.11/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (25.3.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.2)\n",
            "Requirement already satisfied: automat>=24.8.0 in /usr/local/lib/python3.11/dist-packages (from twisted>=21.7.0->scrapy) (25.4.16)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.11/dist-packages (from twisted>=21.7.0->scrapy) (23.10.4)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.11/dist-packages (from twisted>=21.7.0->scrapy) (21.0.0)\n",
            "Requirement already satisfied: incremental>=24.7.0 in /usr/local/lib/python3.11/dist-packages (from twisted>=21.7.0->scrapy) (24.7.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from twisted>=21.7.0->scrapy) (4.13.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope-interface>=5.1.0->scrapy) (75.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2025.4.26)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.18.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.newsday.co.zw/category/4/business> (referer: https://www.newsday.co.zw)\n",
            "2025-05-12 13:06:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.newsday.co.zw/category/4/business> (referer: https://www.newsday.co.zw)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYPtWSw6UgGn",
        "outputId": "93f94e58-583d-4e6b-831a-8822717de458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "2025-05-12 13:06:45 [numexpr.utils] INFO: NumExpr defaulting to 2 threads.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ako1ElMUvLG",
        "outputId": "b8814b60-33ae-4bec-a20e-14e01667a112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NGROK_AUTHTOKEN has been set to: 2wjGixesK4lSkf6H5k7LJNDzgTz_67GdfYACNFqAPWX24BWBt\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IvjVjePiU-An",
        "outputId": "a6f09e78-3573-43cc-ac38-e9bb9e97bb77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.11/dist-packages (2.13.0)\n",
            "Requirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (43.0.3)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.3.0)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.7.1)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.11.0)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.3.2)\n",
            "Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.10.0)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.4.0)\n",
            "Requirement already satisfied: pydispatcher>=2.0.5 in /usr/local/lib/python3.11/dist-packages (from scrapy) (2.0.7)\n",
            "Requirement already satisfied: pyopenssl>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.1)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.8.0)\n",
            "Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.0)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.3.0)\n",
            "Requirement already satisfied: twisted>=21.7.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.11.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (2.3.1)\n",
            "Requirement already satisfied: zope-interface>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (7.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.11/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (25.3.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.2)\n",
            "Requirement already satisfied: automat>=24.8.0 in /usr/local/lib/python3.11/dist-packages (from twisted>=21.7.0->scrapy) (25.4.16)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.11/dist-packages (from twisted>=21.7.0->scrapy) (23.10.4)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.11/dist-packages (from twisted>=21.7.0->scrapy) (21.0.0)\n",
            "Requirement already satisfied: incremental>=24.7.0 in /usr/local/lib/python3.11/dist-packages (from twisted>=21.7.0->scrapy) (24.7.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from twisted>=21.7.0->scrapy) (4.13.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope-interface>=5.1.0->scrapy) (75.2.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (2.32.3)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.18.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "pip install scrapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "DdILpf8ZUvp0",
        "outputId": "d71e6278-0231-42e4-dacf-663e5d7fec77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:scrapy.utils.log:Scrapy 2.13.0 started (bot: scrapybot)\n",
            "2025-05-12 13:08:01 [scrapy.utils.log] INFO: Scrapy 2.13.0 started (bot: scrapybot)\n",
            "INFO:scrapy.utils.log:Versions:\n",
            "{'lxml': '5.4.0',\n",
            " 'libxml2': '2.13.8',\n",
            " 'cssselect': '1.3.0',\n",
            " 'parsel': '1.10.0',\n",
            " 'w3lib': '2.3.1',\n",
            " 'Twisted': '24.11.0',\n",
            " 'Python': '3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]',\n",
            " 'pyOpenSSL': '24.2.1 (OpenSSL 3.3.2 3 Sep 2024)',\n",
            " 'cryptography': '43.0.3',\n",
            " 'Platform': 'Linux-6.1.123+-x86_64-with-glibc2.35'}\n",
            "2025-05-12 13:08:01 [scrapy.utils.log] INFO: Versions:\n",
            "{'lxml': '5.4.0',\n",
            " 'libxml2': '2.13.8',\n",
            " 'cssselect': '1.3.0',\n",
            " 'parsel': '1.10.0',\n",
            " 'w3lib': '2.3.1',\n",
            " 'Twisted': '24.11.0',\n",
            " 'Python': '3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]',\n",
            " 'pyOpenSSL': '24.2.1 (OpenSSL 3.3.2 3 Sep 2024)',\n",
            " 'cryptography': '43.0.3',\n",
            " 'Platform': 'Linux-6.1.123+-x86_64-with-glibc2.35'}\n",
            "INFO:scrapy.addons:Enabled addons:\n",
            "[]\n",
            "2025-05-12 13:08:01 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "DEBUG:scrapy.utils.log:Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "DEBUG:scrapy.utils.log:Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "INFO:scrapy.extensions.telnet:Telnet Password: c2784630151c6620\n",
            "2025-05-12 13:08:01 [scrapy.extensions.telnet] INFO: Telnet Password: c2784630151c6620\n",
            "/usr/local/lib/python3.11/dist-packages/scrapy/extensions/feedexport.py:455: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
            "  exporter = cls(crawler)\n",
            "INFO:scrapy.middleware:Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2025-05-12 13:08:01 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "INFO:scrapy.crawler:Overridden settings:\n",
            "{'CONCURRENT_REQUESTS_PER_DOMAIN': 2,\n",
            " 'DEPTH_LIMIT': 1,\n",
            " 'DOWNLOAD_DELAY': 1,\n",
            " 'FEED_EXPORT_FIELDS': ['title',\n",
            "                        'url',\n",
            "                        'section_name',\n",
            "                        'category',\n",
            "                        'source',\n",
            "                        'timestamp',\n",
            "                        'section_url'],\n",
            " 'LOG_LEVEL': 'INFO',\n",
            " 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
            "               '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
            "2025-05-12 13:08:01 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'CONCURRENT_REQUESTS_PER_DOMAIN': 2,\n",
            " 'DEPTH_LIMIT': 1,\n",
            " 'DOWNLOAD_DELAY': 1,\n",
            " 'FEED_EXPORT_FIELDS': ['title',\n",
            "                        'url',\n",
            "                        'section_name',\n",
            "                        'category',\n",
            "                        'source',\n",
            "                        'timestamp',\n",
            "                        'section_url'],\n",
            " 'LOG_LEVEL': 'INFO',\n",
            " 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
            "               '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Scrapy spider (NewsSpider)...\n",
            "Output will be 'enhanced_news_articles.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:scrapy.middleware:Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2025-05-12 13:08:01 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "INFO:scrapy.middleware:Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n",
            " 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2025-05-12 13:08:01 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n",
            " 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "INFO:scrapy.middleware:Enabled item pipelines:\n",
            "[]\n",
            "2025-05-12 13:08:01 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "This event loop is already running",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a92394fc5c22>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m     })\n\u001b[1;32m    208\u001b[0m     \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNewsSpider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This will block until the crawl is finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Scrapy spider (NewsSpider) finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scrapy/crawler.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[1;32m    500\u001b[0m                 \u001b[0;34m\"after\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"startup\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstall_shutdown_handlers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signal_shutdown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             )\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstall_signal_handlers\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_graceful_stop_reactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDeferred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/twisted/internet/asyncioreactor.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asyncioEventloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_forever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_justStopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_justStopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_forever\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;34m\"\"\"Run until stop() is called.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_coroutine_origin_tracking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_debug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/base_events.py\u001b[0m in \u001b[0;36m_check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This event loop is already running'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
          ]
        }
      ],
      "source": [
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from scrapy.linkextractors import LinkExtractor\n",
        "from datetime import datetime\n",
        "import re\n",
        "import logging\n",
        "\n",
        "class NewsSpider(scrapy.Spider):\n",
        "    name = 'enhanced_news_spider'\n",
        "    custom_settings = {\n",
        "        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'DOWNLOAD_DELAY': 1,\n",
        "        'CONCURRENT_REQUESTS_PER_DOMAIN': 2,\n",
        "        'FEED_FORMAT': 'csv',\n",
        "        'FEED_URI': 'enhanced_news_articles.csv', # Output file\n",
        "        'DEPTH_LIMIT': 1,\n",
        "        'FEED_EXPORT_FIELDS': ['title', 'url', 'section_name', 'category', 'source', 'timestamp', 'section_url'],\n",
        "        'LOG_LEVEL': 'INFO',\n",
        "        'FEED_OVERWRITE': True, # Overwrite the CSV if it exists\n",
        "    }\n",
        "\n",
        "    SECTION_MAPPING = {\n",
        "        'business': ['business', 'economy', 'finance', 'markets', 'money', 'invest', 'stocks', 'companies', 'corporate', 'trade'],\n",
        "        'politics': ['politics', 'government', 'election', 'parliament', 'congress', 'democracy'],\n",
        "        'sports': ['sports', 'football', 'soccer', 'basketball', 'tennis', 'golf', 'olympics'],\n",
        "        'culture': ['culture', 'arts', 'entertainment', 'movies', 'music', 'television', 'theatre', 'gallery', 'celebrities']\n",
        "    }\n",
        "\n",
        "    ALTERNATIVE_CHANNELS = {\n",
        "        'business': ['https://www.bbc.com/news/business', 'https://edition.cnn.com/business', 'https://www.aljazeera.com/economy/'],\n",
        "        'politics': ['https://www.aljazeera.com/politics/', 'https://www.theguardian.com/politics'],\n",
        "        'sports': ['https://www.bbc.com/sport', 'https://edition.cnn.com/sport'],\n",
        "        'culture': ['https://www.bbc.com/culture', 'https://www.theguardian.com/uk/culture']\n",
        "    }\n",
        "\n",
        "    start_urls = [\n",
        "        'https://www.bbc.com',\n",
        "        'https://edition.cnn.com',\n",
        "        'https://www.aljazeera.com',\n",
        "        'https://www.theguardian.com/international',\n",
        "        # 'https://www.newsday.co.zw' # You can uncomment this if needed\n",
        "    ]\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(NewsSpider, self).__init__(*args, **kwargs)\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "\n",
        "    def parse(self, response):\n",
        "        if response.meta.get('is_section_page'):\n",
        "            self.logger.info(f\"Extracting articles from section page: {response.url} (Category: {response.meta.get('category')})\")\n",
        "            yield from self.extract_articles(response)\n",
        "        elif response.url in self.start_urls:\n",
        "            self.logger.info(f\"Discovering sections from homepage: {response.url}\")\n",
        "            yield from self.discover_sections(response)\n",
        "        else:\n",
        "            self.logger.warning(f\"Unexpected URL in parse(): {response.url}. Attempting article extraction.\")\n",
        "            if 'source' not in response.meta:\n",
        "                    response.meta['source'] = self.get_source_name(response.url)\n",
        "            if 'category' not in response.meta:\n",
        "                    response.meta['category'] = self.detect_category(response)\n",
        "            if 'potential_section_name' not in response.meta:\n",
        "                    response.meta['potential_section_name'] = response.meta.get('category', 'Unknown Section').capitalize()\n",
        "            yield from self.extract_articles(response)\n",
        "\n",
        "    def discover_sections(self, response):\n",
        "        link_extractor = LinkExtractor(\n",
        "            restrict_css='nav, ul.menu, div.navigation, header nav, footer nav, .primary-nav, #orb-nav-links',\n",
        "            deny_extensions=['jpg', 'png', 'pdf', 'mp4', 'mp3', 'zip', 'gz', 'css', 'js'],\n",
        "            unique=True\n",
        "        )\n",
        "        found_categories_on_page = set()\n",
        "        current_source_main_domain = response.url.split('//')[-1].split('/')[0]\n",
        "\n",
        "        for link in link_extractor.extract_links(response):\n",
        "            url_lower = link.url.lower()\n",
        "            text_lower = link.text.lower().strip() if link.text else ''\n",
        "            if not text_lower and '#' in link.url: continue\n",
        "\n",
        "            for category, keywords in self.SECTION_MAPPING.items():\n",
        "                if any(kw in url_lower or kw in text_lower for kw in keywords):\n",
        "                    if category not in found_categories_on_page :\n",
        "                        self.logger.info(f\"Found potential section link for '{category}': '{link.text}' ({link.url}) on {response.url}\")\n",
        "                        found_categories_on_page.add(category)\n",
        "                        yield scrapy.Request(\n",
        "                            url=response.urljoin(link.url),\n",
        "                            callback=self.parse,\n",
        "                            meta={\n",
        "                                'category': category,\n",
        "                                'source': self.get_source_name(response.url),\n",
        "                                'potential_section_name': link.text.strip() if link.text else category.capitalize(),\n",
        "                                'is_section_page': True\n",
        "                            }\n",
        "                        )\n",
        "                        break\n",
        "\n",
        "        missing_categories_for_this_source = set(self.SECTION_MAPPING.keys()) - found_categories_on_page\n",
        "        for category in missing_categories_for_this_source:\n",
        "            self.logger.info(f\"No direct link for '{category}' found in navigation of {response.url}. Checking ALTERNATIVE_CHANNELS.\")\n",
        "            for alt_url in self.ALTERNATIVE_CHANNELS.get(category, []):\n",
        "                if current_source_main_domain in alt_url:\n",
        "                    self.logger.info(f\"Attempting alternative URL for '{category}' from {current_source_main_domain}: {alt_url}\")\n",
        "                    yield scrapy.Request(\n",
        "                        url=alt_url, callback=self.parse,\n",
        "                        meta={'category': category, 'source': self.get_source_name(alt_url),\n",
        "                              'potential_section_name': category.capitalize(), 'is_section_page': True}\n",
        "                    )\n",
        "\n",
        "    def extract_articles(self, response):\n",
        "        source = response.meta.get('source', self.get_source_name(response.url))\n",
        "        category = response.meta.get('category', self.detect_category(response))\n",
        "        potential_section_name = response.meta.get('potential_section_name', category.capitalize())\n",
        "        section_name = self.extract_section_name(response, potential_section_name)\n",
        "        if not section_name: section_name = category.capitalize()\n",
        "\n",
        "        article_selectors_map = {\n",
        "            'bbc.com': ['a[type=\"article\"]', 'a[href*=\"/news/articles/\"]', '.gs-c-promo-heading[class*=\"__title\"]'],\n",
        "            'cnn.com': ['a[data-link-type=\"article\"]', '.card a', 'a[href*=\"/videos/\"]'],\n",
        "            'aljazeera.com': ['a.u-clickable-card__link', 'article h3 a', 'a.article-trending__title-link'],\n",
        "            'theguardian.com': ['a[data-link-name=\"article\"]', '.fc-item__link', 'a[aria-label*=\"article\"]']\n",
        "        }\n",
        "        selector_list_for_domain = []\n",
        "        for domain_key in article_selectors_map:\n",
        "            if domain_key in response.url: selector_list_for_domain = article_selectors_map[domain_key]; break\n",
        "\n",
        "        generic_selectors = ['article a[href]', 'div[class*=\"article\"] a[href]', 'a[href*=\"/article/\"]']\n",
        "        combined_selectors = selector_list_for_domain + generic_selectors\n",
        "\n",
        "        articles_found_on_page = set(); article_count = 0\n",
        "        for selector_str in combined_selectors:\n",
        "            if article_count >= 10: break\n",
        "            for article_element in response.css(selector_str):\n",
        "                if article_count >= 10: break\n",
        "                href = article_element.css('::attr(href)').get()\n",
        "                if not href: continue\n",
        "                full_url = response.urljoin(href)\n",
        "                if full_url in articles_found_on_page or not (full_url.startswith('http://') or full_url.startswith('https://')): continue\n",
        "\n",
        "                title = \" \".join(part.strip() for part in article_element.css('::text').getall() if part.strip()).strip()\n",
        "                title = re.sub(r'\\s+', ' ', title).strip()\n",
        "                if len(title) < 10:\n",
        "                    h_texts = article_element.css('h1::text, h2::text, h3::text, h4::text, span[class*=\"title\"]::text').getall()\n",
        "                    if h_texts: title = \" \".join(h.strip() for h in h_texts if h.strip()).strip()\n",
        "                if not title or len(title) < 10 or len(title) > 200: continue\n",
        "\n",
        "                articles_found_on_page.add(full_url)\n",
        "                yield {'title': title, 'url': full_url, 'section_name': section_name, 'category': category,\n",
        "                       'source': source, 'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                       'section_url': response.url}\n",
        "                article_count += 1\n",
        "        if article_count == 0: self.logger.info(f\"No articles extracted from {response.url} (Cat: {category})\")\n",
        "\n",
        "    def extract_section_name(self, response, fallback_name):\n",
        "        selectors = ['h1.section-header__title::text', 'h1[class*=\"PageTitle\"]::text', '.page-title ::text', 'h1::text']\n",
        "        section_name_str = ''\n",
        "        for sel in selectors:\n",
        "            for name_part in response.css(sel).getall():\n",
        "                cleaned_name = name_part.strip()\n",
        "                if cleaned_name: section_name_str = cleaned_name; break\n",
        "            if section_name_str: break\n",
        "        if not section_name_str.strip(): section_name_str = fallback_name\n",
        "        section_name_str = re.sub(r'[^\\w\\s-]', '', section_name_str).strip()\n",
        "        if not section_name_str:\n",
        "            path_parts = [p for p in response.url.split('/') if p]\n",
        "            generic = {'www', 'com', 'news', 'article', 'category'}\n",
        "            for part in reversed(path_parts[-3:]):\n",
        "                cleaned = part.split('.')[0]\n",
        "                if cleaned and len(cleaned) > 3 and cleaned.lower() not in generic and not cleaned.isdigit():\n",
        "                    section_name_str = cleaned.replace('-', ' ').replace('_', ' ').title(); break\n",
        "        return section_name_str[:100] if section_name_str else fallback_name\n",
        "\n",
        "    def get_source_name(self, url):\n",
        "        domain_map = {'bbc.com': 'BBC', 'cnn.com': 'CNN', 'aljazeera.com': 'Al Jazeera', 'theguardian.com': 'The Guardian'}\n",
        "        for domain, name in domain_map.items():\n",
        "            if domain in url: return name\n",
        "        try: return url.split('//')[-1].split('/')[0].replace('www.', '').split('.')[0].capitalize()\n",
        "        except: return 'Unknown Source'\n",
        "\n",
        "    def detect_category(self, response):\n",
        "        if response.meta.get('category'): return response.meta.get('category')\n",
        "        signals = [response.url.lower(), response.css('title::text').get('').lower(),\n",
        "                   ' '.join(response.css('h1::text, h2::text').getall()).lower(),\n",
        "                   response.css('meta[property=\"og:section\"]::attr(content)').get('').lower()]\n",
        "        path_segments = [seg for seg in response.url.lower().split('/') if seg]\n",
        "        for seg in path_segments:\n",
        "            for cat, kws in self.SECTION_MAPPING.items():\n",
        "                if seg in kws or cat == seg: return cat\n",
        "        for signal in signals:\n",
        "            if not signal: continue\n",
        "            for cat, kws in self.SECTION_MAPPING.items():\n",
        "                if any(kw in signal for kw in kws): return cat\n",
        "        return 'general'\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting Scrapy spider (NewsSpider)...\")\n",
        "    print(\"Output will be 'enhanced_news_articles.csv'\")\n",
        "    # Note: FEED_OVERWRITE is in custom_settings, so it will be used.\n",
        "    process = CrawlerProcess(settings={ # Ensure settings are passed if not fully relying on custom_settings\n",
        "        'LOG_LEVEL': 'INFO', # Can be DEBUG for more verbosity\n",
        "        'USER_AGENT': NewsSpider.custom_settings['USER_AGENT'], # Example of ensuring it's passed\n",
        "        'DOWNLOAD_DELAY': NewsSpider.custom_settings['DOWNLOAD_DELAY'],\n",
        "        'CONCURRENT_REQUESTS_PER_DOMAIN': NewsSpider.custom_settings['CONCURRENT_REQUESTS_PER_DOMAIN'],\n",
        "        'DEPTH_LIMIT': NewsSpider.custom_settings['DEPTH_LIMIT'],\n",
        "        'FEED_FORMAT': NewsSpider.custom_settings['FEED_FORMAT'],\n",
        "        'FEED_URI': NewsSpider.custom_settings['FEED_URI'],\n",
        "        'FEED_EXPORT_FIELDS': NewsSpider.custom_settings['FEED_EXPORT_FIELDS'],\n",
        "        'FEED_OVERWRITE': True,\n",
        "    })\n",
        "    process.crawl(NewsSpider)\n",
        "    process.start() # This will block until the crawl is finished\n",
        "    print(\"Scrapy spider (NewsSpider) finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKx8zITDoCAD",
        "outputId": "c51c9937-1685-4595-950d-4cc589505b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxBJpFHloGQy",
        "outputId": "abb43905-b17f-4aa2-e8a9-d8b000718b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:scrapy.utils.log:Scrapy 2.13.0 started (bot: scrapybot)\n",
            "2025-05-12 13:12:52 [scrapy.utils.log] INFO: Scrapy 2.13.0 started (bot: scrapybot)\n",
            "INFO:scrapy.utils.log:Versions:\n",
            "{'lxml': '5.4.0',\n",
            " 'libxml2': '2.13.8',\n",
            " 'cssselect': '1.3.0',\n",
            " 'parsel': '1.10.0',\n",
            " 'w3lib': '2.3.1',\n",
            " 'Twisted': '24.11.0',\n",
            " 'Python': '3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]',\n",
            " 'pyOpenSSL': '24.2.1 (OpenSSL 3.3.2 3 Sep 2024)',\n",
            " 'cryptography': '43.0.3',\n",
            " 'Platform': 'Linux-6.1.123+-x86_64-with-glibc2.35'}\n",
            "2025-05-12 13:12:52 [scrapy.utils.log] INFO: Versions:\n",
            "{'lxml': '5.4.0',\n",
            " 'libxml2': '2.13.8',\n",
            " 'cssselect': '1.3.0',\n",
            " 'parsel': '1.10.0',\n",
            " 'w3lib': '2.3.1',\n",
            " 'Twisted': '24.11.0',\n",
            " 'Python': '3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]',\n",
            " 'pyOpenSSL': '24.2.1 (OpenSSL 3.3.2 3 Sep 2024)',\n",
            " 'cryptography': '43.0.3',\n",
            " 'Platform': 'Linux-6.1.123+-x86_64-with-glibc2.35'}\n",
            "INFO:scrapy.addons:Enabled addons:\n",
            "[]\n",
            "2025-05-12 13:12:52 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "DEBUG:scrapy.utils.log:Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "DEBUG:scrapy.utils.log:Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "INFO:scrapy.extensions.telnet:Telnet Password: 94b3d7923f954a89\n",
            "2025-05-12 13:12:52 [scrapy.extensions.telnet] INFO: Telnet Password: 94b3d7923f954a89\n",
            "/usr/local/lib/python3.11/dist-packages/scrapy/extensions/feedexport.py:455: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
            "  exporter = cls(crawler)\n",
            "INFO:scrapy.middleware:Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2025-05-12 13:12:52 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "INFO:scrapy.crawler:Overridden settings:\n",
            "{'CONCURRENT_REQUESTS_PER_DOMAIN': 2,\n",
            " 'DEPTH_LIMIT': 1,\n",
            " 'DOWNLOAD_DELAY': 1,\n",
            " 'FEED_EXPORT_FIELDS': ['title',\n",
            "                        'url',\n",
            "                        'section_name',\n",
            "                        'category',\n",
            "                        'source',\n",
            "                        'timestamp',\n",
            "                        'section_url'],\n",
            " 'LOG_LEVEL': 'INFO',\n",
            " 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
            "               '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
            "2025-05-12 13:12:52 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'CONCURRENT_REQUESTS_PER_DOMAIN': 2,\n",
            " 'DEPTH_LIMIT': 1,\n",
            " 'DOWNLOAD_DELAY': 1,\n",
            " 'FEED_EXPORT_FIELDS': ['title',\n",
            "                        'url',\n",
            "                        'section_name',\n",
            "                        'category',\n",
            "                        'source',\n",
            "                        'timestamp',\n",
            "                        'section_url'],\n",
            " 'LOG_LEVEL': 'INFO',\n",
            " 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
            "               '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Scrapy spider (NewsSpider)...\n",
            "Output will be 'enhanced_news_articles.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:scrapy.middleware:Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2025-05-12 13:12:52 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "INFO:scrapy.middleware:Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n",
            " 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2025-05-12 13:12:52 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n",
            " 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "INFO:scrapy.middleware:Enabled item pipelines:\n",
            "[]\n",
            "2025-05-12 13:12:52 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "INFO:scrapy.core.engine:Spider opened\n",
            "2025-05-12 13:12:52 [scrapy.core.engine] INFO: Spider opened\n",
            "INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2025-05-12 13:12:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6023\n",
            "2025-05-12 13:12:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.theguardian.com/international> (referer: None)\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.bbc.com> (referer: None)\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.aljazeera.com> (referer: None)\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://edition.cnn.com> (referer: None)\n",
            "INFO:enhanced_news_spider:Discovering sections from homepage: https://www.theguardian.com/international\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Discovering sections from homepage: https://www.theguardian.com/international\n",
            "INFO:enhanced_news_spider:Found potential section link for 'culture': 'Culture' (https://www.theguardian.com/culture) on https://www.theguardian.com/international\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Found potential section link for 'culture': 'Culture' (https://www.theguardian.com/culture) on https://www.theguardian.com/international\n",
            "INFO:enhanced_news_spider:Found potential section link for 'politics': 'US politics' (https://www.theguardian.com/us-news/us-politics) on https://www.theguardian.com/international\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Found potential section link for 'politics': 'US politics' (https://www.theguardian.com/us-news/us-politics) on https://www.theguardian.com/international\n",
            "INFO:enhanced_news_spider:Found potential section link for 'sports': 'Football' (https://www.theguardian.com/football) on https://www.theguardian.com/international\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Found potential section link for 'sports': 'Football' (https://www.theguardian.com/football) on https://www.theguardian.com/international\n",
            "INFO:enhanced_news_spider:Found potential section link for 'business': 'Business' (https://www.theguardian.com/business) on https://www.theguardian.com/international\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Found potential section link for 'business': 'Business' (https://www.theguardian.com/business) on https://www.theguardian.com/international\n",
            "INFO:enhanced_news_spider:Discovering sections from homepage: https://www.bbc.com\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Discovering sections from homepage: https://www.bbc.com\n",
            "INFO:enhanced_news_spider:Found potential section link for 'business': 'Business' (https://www.bbc.com/business) on https://www.bbc.com\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Found potential section link for 'business': 'Business' (https://www.bbc.com/business) on https://www.bbc.com\n",
            "INFO:enhanced_news_spider:Discovering sections from homepage: https://www.aljazeera.com\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Discovering sections from homepage: https://www.aljazeera.com\n",
            "INFO:enhanced_news_spider:Found potential section link for 'sports': 'Sport' (https://www.aljazeera.com/sports/) on https://www.aljazeera.com\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Found potential section link for 'sports': 'Sport' (https://www.aljazeera.com/sports/) on https://www.aljazeera.com\n",
            "INFO:enhanced_news_spider:Discovering sections from homepage: https://edition.cnn.com\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Discovering sections from homepage: https://edition.cnn.com\n",
            "INFO:enhanced_news_spider:Found potential section link for 'politics': '\n",
            "                  \n",
            "                  Politics\n",
            "                ' (https://edition.cnn.com/politics) on https://edition.cnn.com\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Found potential section link for 'politics': '\n",
            "                  \n",
            "                  Politics\n",
            "                ' (https://edition.cnn.com/politics) on https://edition.cnn.com\n",
            "INFO:enhanced_news_spider:Found potential section link for 'culture': 'Culture' (https://www.bbc.com/culture) on https://www.bbc.com\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Found potential section link for 'culture': 'Culture' (https://www.bbc.com/culture) on https://www.bbc.com\n",
            "INFO:enhanced_news_spider:No direct link for 'politics' found in navigation of https://www.bbc.com. Checking ALTERNATIVE_CHANNELS.\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: No direct link for 'politics' found in navigation of https://www.bbc.com. Checking ALTERNATIVE_CHANNELS.\n",
            "INFO:enhanced_news_spider:No direct link for 'sports' found in navigation of https://www.bbc.com. Checking ALTERNATIVE_CHANNELS.\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: No direct link for 'sports' found in navigation of https://www.bbc.com. Checking ALTERNATIVE_CHANNELS.\n",
            "INFO:enhanced_news_spider:Attempting alternative URL for 'sports' from www.bbc.com: https://www.bbc.com/sport\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Attempting alternative URL for 'sports' from www.bbc.com: https://www.bbc.com/sport\n",
            "INFO:enhanced_news_spider:Found potential section link for 'business': 'Economy' (https://www.aljazeera.com/economy/) on https://www.aljazeera.com\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Found potential section link for 'business': 'Economy' (https://www.aljazeera.com/economy/) on https://www.aljazeera.com\n",
            "INFO:enhanced_news_spider:Found potential section link for 'culture': 'In Pictures' (https://www.aljazeera.com/gallery/) on https://www.aljazeera.com\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Found potential section link for 'culture': 'In Pictures' (https://www.aljazeera.com/gallery/) on https://www.aljazeera.com\n",
            "INFO:enhanced_news_spider:No direct link for 'politics' found in navigation of https://www.aljazeera.com. Checking ALTERNATIVE_CHANNELS.\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: No direct link for 'politics' found in navigation of https://www.aljazeera.com. Checking ALTERNATIVE_CHANNELS.\n",
            "INFO:enhanced_news_spider:Attempting alternative URL for 'politics' from www.aljazeera.com: https://www.aljazeera.com/politics/\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Attempting alternative URL for 'politics' from www.aljazeera.com: https://www.aljazeera.com/politics/\n",
            "INFO:enhanced_news_spider:Found potential section link for 'business': '\n",
            "                  \n",
            "                  Business\n",
            "                ' (https://edition.cnn.com/business) on https://edition.cnn.com\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Found potential section link for 'business': '\n",
            "                  \n",
            "                  Business\n",
            "                ' (https://edition.cnn.com/business) on https://edition.cnn.com\n",
            "INFO:enhanced_news_spider:Found potential section link for 'culture': '\n",
            "                  \n",
            "                  Entertainment\n",
            "                ' (https://edition.cnn.com/entertainment) on https://edition.cnn.com\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Found potential section link for 'culture': '\n",
            "                  \n",
            "                  Entertainment\n",
            "                ' (https://edition.cnn.com/entertainment) on https://edition.cnn.com\n",
            "INFO:enhanced_news_spider:Found potential section link for 'sports': '\n",
            "                  \n",
            "                  Sports\n",
            "                ' (https://edition.cnn.com/sports) on https://edition.cnn.com\n",
            "2025-05-12 13:12:53 [enhanced_news_spider] INFO: Found potential section link for 'sports': '\n",
            "                  \n",
            "                  Sports\n",
            "                ' (https://edition.cnn.com/sports) on https://edition.cnn.com\n",
            "DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (302) to <GET https://www.theguardian.com/uk/business> from <GET https://www.theguardian.com/business>\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.aljazeera.com/sports/> (referer: https://www.aljazeera.com)\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://edition.cnn.com/politics> (referer: https://edition.cnn.com)\n",
            "INFO:enhanced_news_spider:Extracting articles from section page: https://www.aljazeera.com/sports/ (Category: sports)\n",
            "2025-05-12 13:12:54 [enhanced_news_spider] INFO: Extracting articles from section page: https://www.aljazeera.com/sports/ (Category: sports)\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/sports/>\n",
            "{'title': 'Reports: Alonso set to take over at Real Madrid before FIFA Club World Cup', 'url': 'https://www.aljazeera.com/sports/2025/5/12/alonso-to-replace-ancelotti-as-real-madrid-manager-reports', 'section_name': 'Sport', 'category': 'sports', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.aljazeera.com/sports/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/sports/>\n",
            "{'title': 'What to know about Virat Kohli’s retirement from Test cricket', 'url': 'https://www.aljazeera.com/sports/2025/5/12/why-did-virat-kohli-retire-from-tests-and-what-does-269-mean', 'section_name': 'Sport', 'category': 'sports', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.aljazeera.com/sports/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/sports/>\n",
            "{'title': 'India great Virat Kohli follows Rohit Sharma into Test cricket retirement', 'url': 'https://www.aljazeera.com/sports/2025/5/12/virat-kohli-announces-retirement-from-test-cricket', 'section_name': 'Sport', 'category': 'sports', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.aljazeera.com/sports/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/sports/>\n",
            "{'title': 'Mbappe hat-trick not enough to stop Barcelona’s La Liga charge', 'url': 'https://www.aljazeera.com/sports/2025/5/11/barcelona-beat-real-madrid-despite-mbappe-hat-trick-as-la-liga-title-nears', 'section_name': 'Sport', 'category': 'sports', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.aljazeera.com/sports/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/sports/>\n",
            "{'title': 'Merino scores then sees red as Arsenal come back to draw 2-2 at Liverpool', 'url': 'https://www.aljazeera.com/sports/2025/5/11/merino-scores-then-sees-red-as-arsenal-come-back-to-draw-2-2-at-liverpool', 'section_name': 'Sport', 'category': 'sports', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.aljazeera.com/sports/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/sports/>\n",
            "{'title': 'Arsenal come back from two goals down to salvage draw with Liverpool', 'url': 'https://www.aljazeera.com/sports/liveblog/2025/5/11/live-liverpool-vs-arsenal-english-premier-league', 'section_name': 'Sport', 'category': 'sports', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.aljazeera.com/sports/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/sports/>\n",
            "{'title': 'Barca beat Real 4-3 to move seven points clear in LaLiga – updates', 'url': 'https://www.aljazeera.com/sports/liveblog/2025/5/11/live-barcelona-vs-real-madrid-la-liga-el-clasico', 'section_name': 'Sport', 'category': 'sports', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.aljazeera.com/sports/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/sports/>\n",
            "{'title': 'NBA playoffs: Timberwolves overcome Warriors late to take 2-1 series lead', 'url': 'https://www.aljazeera.com/sports/2025/5/11/edwards-timberwolves-overcome-warriors-in-game-3-to-take-2-1-series-lead', 'section_name': 'Sport', 'category': 'sports', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.aljazeera.com/sports/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/sports/>\n",
            "{'title': 'Jack Della Maddalena ends Belal Muhammad’s welterweight reign at UFC 315', 'url': 'https://www.aljazeera.com/sports/2025/5/11/ufc-315-della-maddalena-defeats-muhammad-in-welterweight-title-fight', 'section_name': 'Sport', 'category': 'sports', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.aljazeera.com/sports/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/sports/>\n",
            "{'title': 'Ancelotti coy on Real Madrid future, but says Alonso set for ‘big club’', 'url': 'https://www.aljazeera.com/sports/2025/5/10/barcelona-vs-real-madrid-ancelotti-coy-on-future-as-alonso-link-grows', 'section_name': 'Sport', 'category': 'sports', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.aljazeera.com/sports/'}\n",
            "INFO:enhanced_news_spider:Extracting articles from section page: https://edition.cnn.com/politics (Category: politics)\n",
            "2025-05-12 13:12:54 [enhanced_news_spider] INFO: Extracting articles from section page: https://edition.cnn.com/politics (Category: politics)\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/politics>\n",
            "{'title': 'Divided Supreme Court on full display heading into birthright citizenship hearing', 'url': 'https://edition.cnn.com/2025/05/12/politics/supreme-court-divisions-birthright-citizenship-hearing', 'section_name': 'Politics', 'category': 'politics', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://edition.cnn.com/politics'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/politics>\n",
            "{'title': 'Trump is Middle East-bound for his first major international trip of his second term. Here’s what to watch', 'url': 'https://edition.cnn.com/2025/05/12/politics/trump-middle-east-what-to-watch', 'section_name': 'Politics', 'category': 'politics', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://edition.cnn.com/politics'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/politics>\n",
            "{'title': 'What the House GOP is proposing in its first draft of Trump’s sweeping tax and spending cuts package', 'url': 'https://edition.cnn.com/2025/05/02/politics/what-is-in-trump-tax-spending-cuts-package', 'section_name': 'Politics', 'category': 'politics', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://edition.cnn.com/politics'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/politics>\n",
            "{'title': 'GOP opts for Medicaid compromise in battle between centrists and hardliners', 'url': 'https://edition.cnn.com/2025/05/11/politics/house-gop-budget-bill-medicaid', 'section_name': 'Politics', 'category': 'politics', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://edition.cnn.com/politics'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/politics>\n",
            "{'title': 'Trump announces he’ll sign executive order that aims to cut drug prices', 'url': 'https://edition.cnn.com/2025/05/11/politics/trump-prescription-drug-prices', 'section_name': 'Politics', 'category': 'politics', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://edition.cnn.com/politics'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/politics>\n",
            "{'title': 'Trump says US government plans to accept luxury jet following reports of multimillion-dollar gift from Qatar', 'url': 'https://edition.cnn.com/2025/05/11/politics/trump-luxury-jet-qatar-air-force-one', 'section_name': 'Politics', 'category': 'politics', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://edition.cnn.com/politics'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/politics>\n",
            "{'title': 'Democrats involved in tense encounter at ICE facility say Trump administration is trying to intimidate them', 'url': 'https://edition.cnn.com/2025/05/11/politics/ice-facility-democrats-arrest-newark-mayor-baraka', 'section_name': 'Politics', 'category': 'politics', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://edition.cnn.com/politics'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/politics>\n",
            "{'title': 'Saul Loeb/AFP/Getty Images', 'url': 'https://edition.cnn.com/2025/05/12/politics/trump-china-ukraine-canada-us-world-relations', 'section_name': 'Politics', 'category': 'politics', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://edition.cnn.com/politics'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/politics>\n",
            "{'title': 'Trump wants to upend American holidays', 'url': 'https://edition.cnn.com/2025/05/11/politics/victory-day-army-military-parade-trump', 'section_name': 'Politics', 'category': 'politics', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://edition.cnn.com/politics'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/politics>\n",
            "{'title': 'Trump made historic gains with minority voters in 2024. They are already pulling back in 2025', 'url': 'https://edition.cnn.com/2025/05/11/politics/gop-voter-coalition-2024-trump', 'section_name': 'Politics', 'category': 'politics', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://edition.cnn.com/politics'}\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.theguardian.com/football> (referer: https://www.theguardian.com/international)\n",
            "INFO:enhanced_news_spider:Extracting articles from section page: https://www.theguardian.com/football (Category: sports)\n",
            "2025-05-12 13:12:54 [enhanced_news_spider] INFO: Extracting articles from section page: https://www.theguardian.com/football (Category: sports)\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/football>\n",
            "{'title': 'Premier League: 10 talking points from the weekend’s action', 'url': 'https://www.theguardian.com/football/2025/may/12/premier-league-10-talking-points-from-the-weekends-action', 'section_name': 'Football', 'category': 'sports', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.theguardian.com/football'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/football>\n",
            "{'title': 'Napoli fail to make ‘bonus’ game pay as uncertainty swirls at top of Serie A', 'url': 'https://www.theguardian.com/football/2025/may/12/napoli-serie-a-title-race-italy-football', 'section_name': 'Football', 'category': 'sports', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.theguardian.com/football'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/football>\n",
            "{'title': 'Nuno defends Nottingham Forest owner over ‘scandalous’ confrontation', 'url': 'https://www.theguardian.com/football/2025/may/11/nuno-defends-nottingham-forest-owner-over-scandalous-confrontation', 'section_name': 'Football', 'category': 'sports', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.theguardian.com/football'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/football>\n",
            "{'title': 'Como defender prints CV on shirt to highlight post-career difficulties for female players', 'url': 'https://www.theguardian.com/football/2025/may/12/como-defender-prints-cv-on-shirt-to-highlight-post-career-difficulties-for-female-players', 'section_name': 'Football', 'category': 'sports', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.theguardian.com/football'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/football>\n",
            "{'title': 'Triviality of the non-title decider leads to fun, frivolity and petty booing', 'url': 'https://www.theguardian.com/football/2025/may/11/triviality-of-the-non-title-decider-leads-to-fun-frivolity-and-petty-booing', 'section_name': 'Football', 'category': 'sports', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.theguardian.com/football'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/football>\n",
            "{'title': 'David Beckham tells Minnesota to ‘show a little respect’ after Pink Phony Club jibe', 'url': 'https://www.theguardian.com/football/2025/may/12/david-beckham-minnesota-united-mls-inter-miami', 'section_name': 'Football', 'category': 'sports', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.theguardian.com/football'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/football>\n",
            "{'title': 'Amorim questions his United future after Soucek sets up West Ham win', 'url': 'https://www.theguardian.com/football/2025/may/11/manchester-united-west-ham-premier-league-match-report', 'section_name': 'Football', 'category': 'sports', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.theguardian.com/football'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/football>\n",
            "{'title': 'WSL 2024-25 season review: our writers’ best and worst', 'url': 'https://www.theguardian.com/football/2025/may/12/wsl-2024-25-season-review-our-writers-best-and-worst', 'section_name': 'Football', 'category': 'sports', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.theguardian.com/football'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/football>\n",
            "{'title': 'Hansi Flick says Barça ‘don’t feel like champions’ after chaotic clásico', 'url': 'https://www.theguardian.com/football/2025/may/11/hansi-flick-barcelona-real-madrid-clasico-la-liga', 'section_name': 'Football', 'category': 'sports', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.theguardian.com/football'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/football>\n",
            "{'title': 'Live ‘The clock is ticking’: Germany says Russia must respond to ceasefire offer by end of day or face sanctions – Europe live', 'url': 'https://www.theguardian.com/world/live/2025/may/12/russia-ukraine-war-zelenskyy-putin-trump-europe-live-latest-news', 'section_name': 'Football', 'category': 'sports', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:54', 'section_url': 'https://www.theguardian.com/football'}\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.bbc.com/business> (referer: https://www.bbc.com)\n",
            "INFO:enhanced_news_spider:Extracting articles from section page: https://www.bbc.com/business (Category: business)\n",
            "2025-05-12 13:12:55 [enhanced_news_spider] INFO: Extracting articles from section page: https://www.bbc.com/business (Category: business)\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/business>\n",
            "{'title': 'US and China agree to slash tariffs for 90 days US Treasury Secretary Scott Bessent says both countries will lower their reciprocal tariffs by 115%. 1 hr ago Business', 'url': 'https://www.bbc.com/news/articles/czx0ry7kdk5o', 'section_name': 'Business', 'category': 'business', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:55', 'section_url': 'https://www.bbc.com/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/business>\n",
            "{'title': 'The US and China are finally talking. Why now? Economic pressures are forcing Donald Trump and Xi Jinping to seek an off-ramp in the trade war. 2 days ago World', 'url': 'https://www.bbc.com/news/articles/c74qjjvzlgjo', 'section_name': 'Business', 'category': 'business', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:55', 'section_url': 'https://www.bbc.com/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/business>\n",
            "{'title': 'US says \\'deal\\' reached with China after trade talks The US treasury secretary calls the talks \"productive\", while China\\'s vice premier describes them as \"candid\". 7 hrs ago World', 'url': 'https://www.bbc.com/news/articles/cn053edex5eo', 'section_name': 'Business', 'category': 'business', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:55', 'section_url': 'https://www.bbc.com/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/business>\n",
            "{'title': 'Trump heads to Saudi Arabia eyeing more investment in US As the president goes to Saudi Arabia, Qatar and the UAE he wants them to buy more from the States. 13 hrs ago Middle East', 'url': 'https://www.bbc.com/news/articles/crk2me7vjxxo', 'section_name': 'Business', 'category': 'business', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:55', 'section_url': 'https://www.bbc.com/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/business>\n",
            "{'title': \"India worried about Chinese 'dumping' as trade tensions with Trump escalate Experts warn that a flood of cheap Chinese goods could hurt the competitiveness of Indian exports. 7 days ago Asia\", 'url': 'https://www.bbc.com/news/articles/ckg5d505v8xo', 'section_name': 'Business', 'category': 'business', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:55', 'section_url': 'https://www.bbc.com/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/business>\n",
            "{'title': 'Kwik Fit founder Sir Tom Farmer dies aged 84 The Edinburgh-born businessman built the firm into a £1bn business and also owned Hibernian FC. 2 days ago Scotland', 'url': 'https://www.bbc.com/news/articles/cvgn3ngvm8eo', 'section_name': 'Business', 'category': 'business', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:55', 'section_url': 'https://www.bbc.com/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/business>\n",
            "{'title': \"Mexico sues Google over 'Gulf of America' name change President Sheinbaum argues Trump's executive order applies only to the US portion of the continental shelf. 3 days ago World\", 'url': 'https://www.bbc.com/news/articles/c5yk5nj7p7ko', 'section_name': 'Business', 'category': 'business', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:55', 'section_url': 'https://www.bbc.com/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/business>\n",
            "{'title': \"Trump proposes 80% China tariff ahead of trade talks The meeting is the strongest sign yet that the world's two biggest economies are ready to de-escalate their trade war. 3 days ago Business\", 'url': 'https://www.bbc.com/news/articles/c4gkvp6438ko', 'section_name': 'Business', 'category': 'business', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:55', 'section_url': 'https://www.bbc.com/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/business>\n",
            "{'title': \"3 days ago UK to announce fresh sanctions on Putin's 'shadow fleet' Officials say action will be taken against up to 100 oil tankers that have been carrying cargo. 3 days ago UK\", 'url': 'https://www.bbc.com/news/articles/c04e9gwqzyeo', 'section_name': 'Business', 'category': 'business', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:55', 'section_url': 'https://www.bbc.com/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/business>\n",
            "{'title': \"3 days ago What is in the UK-US tariff deal? The two nations have announced a deal to reduce tariffs - here's what you need to know. 3 days ago Business\", 'url': 'https://www.bbc.com/news/articles/c15ng4g5g0eo', 'section_name': 'Business', 'category': 'business', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:55', 'section_url': 'https://www.bbc.com/business'}\n",
            "DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://edition.cnn.com/sport> from <GET https://edition.cnn.com/sports>\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.bbc.com/sport> (referer: https://www.bbc.com)\n",
            "INFO:enhanced_news_spider:Extracting articles from section page: https://www.bbc.com/sport (Category: sports)\n",
            "2025-05-12 13:12:55 [enhanced_news_spider] INFO: Extracting articles from section page: https://www.bbc.com/sport (Category: sports)\n",
            "DEBUG:scrapy.core.engine:Crawled (404) <GET https://www.aljazeera.com/politics/> (referer: https://www.aljazeera.com)\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/sport>\n",
            "{'title': \"'He made batting look like art' - Sharma's surprise retirement\", 'url': 'https://www.bbc.com/news/articles/c071e9dnn85o', 'section_name': 'BBC Sport', 'category': 'sports', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:55', 'section_url': 'https://www.bbc.com/sport'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/sport>\n",
            "{'title': 'Where and how to watch BBC News', 'url': 'https://www.bbc.com/news/world-radio-and-tv-14563857', 'section_name': 'BBC Sport', 'category': 'sports', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:55', 'section_url': 'https://www.bbc.com/sport'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/sport>\n",
            "{'title': 'BBC World News: 24 hour news TV channel', 'url': 'https://www.bbc.com/news/world_radio_and_tv', 'section_name': 'BBC Sport', 'category': 'sports', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:56', 'section_url': 'https://www.bbc.com/sport'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/sport>\n",
            "{'title': \"Live . 'Trophy win can be a turning point' - Spurs Europa League final media day news conference\", 'url': 'https://www.bbc.com/sport/football/live/cy0kl9p2l2kt', 'section_name': 'BBC Sport', 'category': 'sports', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:56', 'section_url': 'https://www.bbc.com/sport'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/sport>\n",
            "{'title': 'India great Kohli retires from Test cricket', 'url': 'https://www.bbc.com/sport/cricket/articles/c0mrzlr919xo', 'section_name': 'BBC Sport', 'category': 'sports', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:56', 'section_url': 'https://www.bbc.com/sport'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/sport>\n",
            "{'title': 'Real Madrid want Alonso in place for Club World Cup', 'url': 'https://www.bbc.com/sport/football/articles/c4g7pk04l5no', 'section_name': 'BBC Sport', 'category': 'sports', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:56', 'section_url': 'https://www.bbc.com/sport'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/sport>\n",
            "{'title': \"Beckham tells Minnesota United to 'show a little respect'\", 'url': 'https://www.bbc.com/sport/football/articles/cy4en087290o', 'section_name': 'BBC Sport', 'category': 'sports', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:56', 'section_url': 'https://www.bbc.com/sport'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/sport>\n",
            "{'title': 'Live . County Championship, Essex hold up Yorkshire, Foakes career-best, day four - radio & text', 'url': 'https://www.bbc.com/sport/cricket/live/cr4n1p290evt', 'section_name': 'BBC Sport', 'category': 'sports', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:56', 'section_url': 'https://www.bbc.com/sport'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/sport>\n",
            "{'title': 'Leicester name Parling as head coach', 'url': 'https://www.bbc.com/sport/rugby-union/articles/c2e37mzvgdno', 'section_name': 'BBC Sport', 'category': 'sports', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:56', 'section_url': 'https://www.bbc.com/sport'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/sport>\n",
            "{'title': 'Why McIlroy is favourite to win US PGA at Quail Hollow', 'url': 'https://www.bbc.com/sport/golf/articles/c8redr0degro', 'section_name': 'BBC Sport', 'category': 'sports', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:56', 'section_url': 'https://www.bbc.com/sport'}\n",
            "INFO:scrapy.spidermiddlewares.httperror:Ignoring response <404 https://www.aljazeera.com/politics/>: HTTP status code is not handled or not allowed\n",
            "2025-05-12 13:12:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.aljazeera.com/politics/>: HTTP status code is not handled or not allowed\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.theguardian.com/us-news/us-politics> (referer: https://www.theguardian.com/international)\n",
            "INFO:enhanced_news_spider:Extracting articles from section page: https://www.theguardian.com/us-news/us-politics (Category: politics)\n",
            "2025-05-12 13:12:56 [enhanced_news_spider] INFO: Extracting articles from section page: https://www.theguardian.com/us-news/us-politics (Category: politics)\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/us-news/us-politics>\n",
            "{'title': 'Trump says planned gift of luxury plane from Qatar is a very ‘transparent’ deal', 'url': 'https://www.theguardian.com/us-news/2025/may/11/trump-accept-luxury-plane-gift-qatar', 'section_name': 'US politics', 'category': 'politics', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:56', 'section_url': 'https://www.theguardian.com/us-news/us-politics'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/us-news/us-politics>\n",
            "{'title': 'Trump might claim China tariff victory – but this is Capitulation Day', 'url': 'https://www.theguardian.com/us-news/2025/may/12/trump-china-tariff-victory-capitulation-day', 'section_name': 'US politics', 'category': 'politics', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:56', 'section_url': 'https://www.theguardian.com/us-news/us-politics'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/us-news/us-politics>\n",
            "{'title': 'RFK Jr’s autism comments place blame and shift research responsibility to parents, critics say', 'url': 'https://www.theguardian.com/us-news/2025/may/12/rfk-jr-autism-comments-blame-parents', 'section_name': 'US politics', 'category': 'politics', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:56', 'section_url': 'https://www.theguardian.com/us-news/us-politics'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/us-news/us-politics>\n",
            "{'title': 'A bicycle, pencil sharpener, 300lb of raw meat: US presidential gifts and the rules governing them', 'url': 'https://www.theguardian.com/us-news/2025/may/12/us-presidential-foreign-gifts-laws-rules-ntwnfb', 'section_name': 'US politics', 'category': 'politics', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:56', 'section_url': 'https://www.theguardian.com/us-news/us-politics'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/us-news/us-politics>\n",
            "{'title': 'Daughter of Maggie Gyllenhaal and Peter Sarsgaard arrested at Columbia University protests', 'url': 'https://www.theguardian.com/us-news/2025/may/10/ramona-daughter-maggie-gyllenhaal-peter-scarsgaard-arrested-columbia-protests', 'section_name': 'US politics', 'category': 'politics', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:56', 'section_url': 'https://www.theguardian.com/us-news/us-politics'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/us-news/us-politics>\n",
            "{'title': 'Trump administration offers refugee status to 49 white South Africans', 'url': 'https://www.theguardian.com/us-news/2025/may/11/trump-administration-south-african-refugees', 'section_name': 'US politics', 'category': 'politics', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:56', 'section_url': 'https://www.theguardian.com/us-news/us-politics'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/us-news/us-politics>\n",
            "{'title': 'Trump officials ‘created confrontation’ that led to arrest of Newark mayor', 'url': 'https://www.theguardian.com/us-news/2025/may/11/trump-officials-newark-mayor-arrest', 'section_name': 'US politics', 'category': 'politics', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:56', 'section_url': 'https://www.theguardian.com/us-news/us-politics'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/us-news/us-politics>\n",
            "{'title': 'Disputed North Carolina race offers playbook for beaten candidates, experts warn', 'url': 'https://www.theguardian.com/us-news/2025/may/12/north-carolina-election-race', 'section_name': 'US politics', 'category': 'politics', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:56', 'section_url': 'https://www.theguardian.com/us-news/us-politics'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/us-news/us-politics>\n",
            "{'title': 'Trump complains the US media aren’t bending to his will. Aren’t they?', 'url': 'https://www.theguardian.com/us-news/2025/may/11/trump-media-journalists-60-minutes', 'section_name': 'US politics', 'category': 'politics', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:56', 'section_url': 'https://www.theguardian.com/us-news/us-politics'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/us-news/us-politics>\n",
            "{'title': 'Live ‘The clock is ticking’: Germany says Russia must respond to ceasefire offer by end of day or face sanctions – Europe live', 'url': 'https://www.theguardian.com/world/live/2025/may/12/russia-ukraine-war-zelenskyy-putin-trump-europe-live-latest-news', 'section_name': 'US politics', 'category': 'politics', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:56', 'section_url': 'https://www.theguardian.com/us-news/us-politics'}\n",
            "DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (302) to <GET https://www.theguardian.com/uk/culture> from <GET https://www.theguardian.com/culture>\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.bbc.com/culture> (referer: https://www.bbc.com)\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.aljazeera.com/gallery/> (referer: https://www.aljazeera.com)\n",
            "INFO:enhanced_news_spider:Extracting articles from section page: https://www.bbc.com/culture (Category: culture)\n",
            "2025-05-12 13:12:57 [enhanced_news_spider] INFO: Extracting articles from section page: https://www.bbc.com/culture (Category: culture)\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/culture>\n",
            "{'title': \"Baftas 2025: A wedding dress, party plans and Alan Cumming's four suits Find out all the backstage drama from this year's Bafta TV awards. 12 hrs ago Culture\", 'url': 'https://www.bbc.com/news/articles/cx2xn47vlv9o', 'section_name': 'Culture', 'category': 'culture', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.bbc.com/culture'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/culture>\n",
            "{'title': 'Bafta TV Awards: The winners list in full Jessica Gunning was among the winners, winning best supporting actress for Baby Reindeer. 17 hrs ago Culture', 'url': 'https://www.bbc.com/news/articles/c4grlp8p264o', 'section_name': 'Culture', 'category': 'culture', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.bbc.com/culture'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/culture>\n",
            "{'title': 'Amber Heard announces birth of twins in Mother\\'s Day post The US actress reveals that she has welcomed daughter Agnes and son Ocean and is \"elated beyond words\". 18 hrs ago Culture', 'url': 'https://www.bbc.com/news/articles/c79eznzx50vo', 'section_name': 'Culture', 'category': 'culture', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.bbc.com/culture'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/culture>\n",
            "{'title': 'How to win Eurovision, according to the experts Eurovision songwriters, judges and more discuss crafting the ideal anthem. 1 day ago Europe', 'url': 'https://www.bbc.com/news/articles/cdr5lvgzz07o', 'section_name': 'Culture', 'category': 'culture', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.bbc.com/culture'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/culture>\n",
            "{'title': 'Mr Bates and Mr Loverman win top Bafta Awards 17 hrs ago Culture', 'url': 'https://www.bbc.com/news/articles/c1dep5ryd3vo', 'section_name': 'Culture', 'category': 'culture', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.bbc.com/culture'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/culture>\n",
            "{'title': 'Billie Piper and Danny Dyer among stars on Baftas red carpet 17 hrs ago Culture', 'url': 'https://www.bbc.com/news/articles/ckg4nge2m45o', 'section_name': 'Culture', 'category': 'culture', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.bbc.com/culture'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/culture>\n",
            "{'title': \"Yungblud: I've thought about becoming a politician 23 hrs ago Culture\", 'url': 'https://www.bbc.com/news/articles/cg5v6530y40o', 'section_name': 'Culture', 'category': 'culture', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.bbc.com/culture'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/culture>\n",
            "{'title': '2 hrs ago Rat sculptures damaged soon after going on display The co-director of the Hull and East Yorkshire project says the vast majority of people \"love them\". 2 hrs ago Hull & East Yorkshire', 'url': 'https://www.bbc.com/news/articles/c1mglpmrrmvo', 'section_name': 'Culture', 'category': 'culture', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.bbc.com/culture'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/culture>\n",
            "{'title': '4 hrs ago Ruth Jones hints at new project with James Corden Ruth Jones, writer and actress of Gavin Stacey, says her Bafta win for playing Nessa is \"immense\". 4 hrs ago Wales', 'url': 'https://www.bbc.com/news/articles/c5yq34458y3o', 'section_name': 'Culture', 'category': 'culture', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.bbc.com/culture'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.bbc.com/culture>\n",
            "{'title': '6 hrs ago Shia LaBeouf drama crew mark Barry filming with tattoos Shia Labeouf, Toby Kebell and Michael Socha star in the boxing drama filmed in Barry and Penarth. 6 hrs ago Wales', 'url': 'https://www.bbc.com/news/articles/c3wd6q0n1xdo', 'section_name': 'Culture', 'category': 'culture', 'source': 'BBC', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.bbc.com/culture'}\n",
            "INFO:enhanced_news_spider:Extracting articles from section page: https://www.aljazeera.com/gallery/ (Category: culture)\n",
            "2025-05-12 13:12:57 [enhanced_news_spider] INFO: Extracting articles from section page: https://www.aljazeera.com/gallery/ (Category: culture)\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/gallery/>\n",
            "{'title': 'History Illustrated: India, Pakistan and the fight for Kashmir', 'url': 'https://www.aljazeera.com/gallery/2025/5/12/india-pakistan-and-the-fight-for-kashmir', 'section_name': 'In Pictures', 'category': 'culture', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/gallery/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/gallery/>\n",
            "{'title': 'Photos: Gaza’s hospitals cannot provide food to recovering patients', 'url': 'https://www.aljazeera.com/gallery/2025/5/12/gazas-hospitals-cannot-provide-food-to-recovering-patients', 'section_name': 'In Pictures', 'category': 'culture', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/gallery/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/gallery/>\n",
            "{'title': 'Photos: Heavy rains cause flooding in Somalia’s Mogadishu, killing seven', 'url': 'https://www.aljazeera.com/gallery/2025/5/11/heavy-rains-cause-flooding-in-somali-capital-killing-seven', 'section_name': 'In Pictures', 'category': 'culture', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/gallery/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/gallery/>\n",
            "{'title': 'Week in Pictures: From India-Pakistan fighting to V-E Day anniversary', 'url': 'https://www.aljazeera.com/gallery/2025/5/11/week-in-pictures-from-india-pakistan-fighting-to-v-e-day-anniversary', 'section_name': 'In Pictures', 'category': 'culture', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/gallery/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/gallery/>\n",
            "{'title': 'Photos: Night of intense bombardment sends people fleeing in Kashmir', 'url': 'https://www.aljazeera.com/gallery/2025/5/9/intense-shelling-in-kashmir-dozens-injured-thousands-seek-shelter', 'section_name': 'In Pictures', 'category': 'culture', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/gallery/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/gallery/>\n",
            "{'title': 'Photos: Russian military parade marks 80 years since victory over Nazis', 'url': 'https://www.aljazeera.com/gallery/2025/5/9/russia-marks-80-years-since-nazi-defeat-with-grand-parade', 'section_name': 'In Pictures', 'category': 'culture', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/gallery/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/gallery/>\n",
            "{'title': 'UN experts warn of ‘annihilation’ as Gaza deaths mount', 'url': 'https://www.aljazeera.com/gallery/2025/5/8/un-experts-warn-of-annihilation-as-gaza-deaths-mount', 'section_name': 'In Pictures', 'category': 'culture', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/gallery/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/gallery/>\n",
            "{'title': 'Photos: Kashmir violence escalates, dozens killed in India-Pakistan fire', 'url': 'https://www.aljazeera.com/gallery/2025/5/8/nowhere-to-go-kashmir-violence-escalates-amid-india-pakistan-crossfire', 'section_name': 'In Pictures', 'category': 'culture', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/gallery/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/gallery/>\n",
            "{'title': 'History Illustrated: Pope Francis and the clash of the conclaves', 'url': 'https://www.aljazeera.com/gallery/2025/5/7/pope-francis-and-the-clash-of-the-conclaves', 'section_name': 'In Pictures', 'category': 'culture', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/gallery/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/gallery/>\n",
            "{'title': 'Key images as India launches military attack and Pakistan hits back', 'url': 'https://www.aljazeera.com/gallery/2025/5/7/india-fired-missiles-into-pakistan', 'section_name': 'In Pictures', 'category': 'culture', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/gallery/'}\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://edition.cnn.com/entertainment> (referer: https://edition.cnn.com)\n",
            "INFO:enhanced_news_spider:Extracting articles from section page: https://edition.cnn.com/entertainment (Category: culture)\n",
            "2025-05-12 13:12:57 [enhanced_news_spider] INFO: Extracting articles from section page: https://edition.cnn.com/entertainment (Category: culture)\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/entertainment>\n",
            "{'title': 'Sean ‘Diddy’ Combs ‘holding faith’ and ready to ‘finally have his side of the story presented,’ says source', 'url': 'https://edition.cnn.com/2025/05/12/entertainment/defense-legal-strategy-sean-diddy-combs', 'section_name': 'Entertainment', 'category': 'culture', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://edition.cnn.com/entertainment'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/entertainment>\n",
            "{'title': 'Cecily Strong and Colin Jost make surprise cameos in ‘SNL’ cold open', 'url': 'https://edition.cnn.com/2025/05/11/entertainment/cecily-strong-colin-jost-snl-opening', 'section_name': 'Entertainment', 'category': 'culture', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://edition.cnn.com/entertainment'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/entertainment>\n",
            "{'title': 'Courtesy of Warner Bros. Pictures', 'url': 'https://edition.cnn.com/2025/05/10/entertainment/clarksdale-mississippi-mayor-excited-sinners-screening', 'section_name': 'Entertainment', 'category': 'culture', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://edition.cnn.com/entertainment'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/entertainment>\n",
            "{'title': 'Matt Winkelmeyer/Getty Images', 'url': 'https://edition.cnn.com/2025/05/09/entertainment/taylor-swift-subpoenaed-lively-baldoni-case', 'section_name': 'Entertainment', 'category': 'culture', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://edition.cnn.com/entertainment'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/entertainment>\n",
            "{'title': 'From Adam Scott/Instagram', 'url': 'https://edition.cnn.com/2025/05/10/entertainment/adam-scott-ike-barinholtz-the-studio', 'section_name': 'Entertainment', 'category': 'culture', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://edition.cnn.com/entertainment'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/entertainment>\n",
            "{'title': 'Ernesto Ruscio/Getty Images', 'url': 'https://edition.cnn.com/2025/05/11/entertainment/amber-heard-twins-announcement-intl-hnk', 'section_name': 'Entertainment', 'category': 'culture', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://edition.cnn.com/entertainment'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/entertainment>\n",
            "{'title': 'Mike Coppola/Getty Images', 'url': 'https://edition.cnn.com/2025/05/09/entertainment/liev-schreiber-daughter-kai', 'section_name': 'Entertainment', 'category': 'culture', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://edition.cnn.com/entertainment'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/entertainment>\n",
            "{'title': 'Skydance/Paramount Pictures and Skydance', 'url': 'https://edition.cnn.com/2025/05/11/entertainment/cannes-film-festival-2025-preview', 'section_name': 'Entertainment', 'category': 'culture', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://edition.cnn.com/entertainment'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/entertainment>\n",
            "{'title': 'The Golden Globes will add a new category recognizing podcasts starting next year', 'url': 'https://edition.cnn.com/2025/05/07/entertainment/golden-globes-podcasts-category', 'section_name': 'Entertainment', 'category': 'culture', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://edition.cnn.com/entertainment'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/entertainment>\n",
            "{'title': 'Jeff Bridges says he’s ‘feeling good’ nearly 5 years after cancer diagnosis, but dealing with ‘long-term’ Covid effects', 'url': 'https://edition.cnn.com/2025/05/10/entertainment/jeff-bridges-cancer-covid-update-wellness', 'section_name': 'Entertainment', 'category': 'culture', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://edition.cnn.com/entertainment'}\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.aljazeera.com/economy/> (referer: https://www.aljazeera.com)\n",
            "INFO:enhanced_news_spider:Extracting articles from section page: https://www.aljazeera.com/economy/ (Category: business)\n",
            "2025-05-12 13:12:57 [enhanced_news_spider] INFO: Extracting articles from section page: https://www.aljazeera.com/economy/ (Category: business)\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/economy/>\n",
            "{'title': '‘I don’t have the cash to pay for these tariffs’: US small biz suffers', 'url': 'https://www.aljazeera.com/economy/2025/5/9/i-dont-have-the-cash-to-pay-for-these-tariffs-us-small-biz-suffers', 'section_name': 'Economy', 'category': 'business', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/economy/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/economy/>\n",
            "{'title': 'Canadian unemployment rate hits six month high amid US imposed tariffs', 'url': 'https://www.aljazeera.com/economy/2025/5/9/canadian-unemployment-rate-hits-six-month-high-amid-us-imposed-tariffs', 'section_name': 'Economy', 'category': 'business', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/economy/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/economy/>\n",
            "{'title': 'US and UK cement trade deal easing recession fears', 'url': 'https://www.aljazeera.com/economy/2025/5/8/us-and-uk-cement-breakthrough-deal-on-trade', 'section_name': 'Economy', 'category': 'business', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/economy/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/economy/>\n",
            "{'title': 'Fact check: Has Canadian tourism to Florida dropped by 80 percent?', 'url': 'https://www.aljazeera.com/news/2025/5/8/fact-check-has-canadian-tourism-to-florida-dropped-by-80-percent', 'section_name': 'Economy', 'category': 'business', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/economy/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/economy/>\n",
            "{'title': 'Trump to announce trade deal with UK on Thursday, reports say', 'url': 'https://www.aljazeera.com/economy/2025/5/8/trump-to-announce-trade-deal-with-uk-reports-say', 'section_name': 'Economy', 'category': 'business', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/economy/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/economy/>\n",
            "{'title': 'The Federal Reserve leaves interest rates unchanged', 'url': 'https://www.aljazeera.com/economy/2025/5/7/us-federal-reserve-holds-rates-steady-warns-of-higher-unemployment', 'section_name': 'Economy', 'category': 'business', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/economy/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/economy/>\n",
            "{'title': 'Ford to raise prices on three cars produced in Mexico: RPT', 'url': 'https://www.aljazeera.com/economy/2025/5/7/ford-to-raise-prices-on-three-cars-produced-in-mexico-report', 'section_name': 'Economy', 'category': 'business', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/economy/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/economy/>\n",
            "{'title': 'Two-thirds of global warming caused by world’s richest 10%, study finds', 'url': 'https://www.aljazeera.com/news/2025/5/7/two-thirds-of-global-warming-caused-by-worlds-richest-10-study-finds', 'section_name': 'Economy', 'category': 'business', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/economy/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/economy/>\n",
            "{'title': 'US, China to hold talks in Switzerland amid Trump’s trade war', 'url': 'https://www.aljazeera.com/economy/2025/5/7/us-china-to-hold-talks-in-switzerland-amid-trumps-trade-war', 'section_name': 'Economy', 'category': 'business', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/economy/'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.aljazeera.com/economy/>\n",
            "{'title': 'WeightWatchers files for bankruptcy amid rise of weight-loss drugs', 'url': 'https://www.aljazeera.com/economy/2025/5/7/weightwatchers-files-for-bankruptcy-amid-rise-of-weight-loss-drugs', 'section_name': 'Economy', 'category': 'business', 'source': 'Al Jazeera', 'timestamp': '2025-05-12 13:12:57', 'section_url': 'https://www.aljazeera.com/economy/'}\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.theguardian.com/uk/business> (referer: https://www.theguardian.com/international)\n",
            "INFO:enhanced_news_spider:Extracting articles from section page: https://www.theguardian.com/uk/business (Category: business)\n",
            "2025-05-12 13:12:58 [enhanced_news_spider] INFO: Extracting articles from section page: https://www.theguardian.com/uk/business (Category: business)\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/business>\n",
            "{'title': 'Live Stock markets, dollar and oil rally as US and China agree to slash tariffs in 90-day pause – business live', 'url': 'https://www.theguardian.com/business/live/2025/may/12/us-china-trade-war-talks-stock-markets-oil-dollar-gold-business-live-news', 'section_name': 'Business', 'category': 'business', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://www.theguardian.com/uk/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/business>\n",
            "{'title': 'China and US agree 90-day pause to trade war initiated by Donald Trump', 'url': 'https://www.theguardian.com/us-news/2025/may/12/china-us-agree-pause-trade-war-trump', 'section_name': 'Business', 'category': 'business', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://www.theguardian.com/uk/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/business>\n",
            "{'title': '‘It’s an illogical job’: Why driving a train isn’t as cushy as it might seem', 'url': 'https://www.theguardian.com/business/2025/may/11/its-an-illogical-job-why-driving-a-train-isnt-as-cushy-as-it-might-seem', 'section_name': 'Business', 'category': 'business', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://www.theguardian.com/uk/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/business>\n",
            "{'title': 'MPs should not accept any murky answers from Thames Water chair on potential sale', 'url': 'https://www.theguardian.com/business/nils-pratley-on-finance/2025/may/12/mps-should-not-accept-any-murky-answers-from-thames-water-chair-on-potential-sale', 'section_name': 'Business', 'category': 'business', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://www.theguardian.com/uk/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/business>\n",
            "{'title': 'Americans putting life on hold amid economic anxiety under Trump, poll shows', 'url': 'https://www.theguardian.com/business/2025/may/12/americans-economy-trump-poll', 'section_name': 'Business', 'category': 'business', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://www.theguardian.com/uk/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/business>\n",
            "{'title': 'UK food shops report ‘massive’ rise in pensioner shoplifting', 'url': 'https://www.theguardian.com/business/2025/may/12/uk-food-shops-report-massive-rise-in-pensioner-shoplifting', 'section_name': 'Business', 'category': 'business', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://www.theguardian.com/uk/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/business>\n",
            "{'title': 'Pharmaceutical stocks slide as Trump vows to cut US prescription drug prices ‘by 30-80%’', 'url': 'https://www.theguardian.com/business/2025/may/12/pharmaceutical-stocks-slide-as-trump-vows-to-cut-prescription-drug-prices-by-30-80', 'section_name': 'Business', 'category': 'business', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://www.theguardian.com/uk/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/business>\n",
            "{'title': 'Staff layoffs in UK gain pace amid Trump tariff turmoil and labour cost increases', 'url': 'https://www.theguardian.com/business/2025/may/12/staff-layoffs-in-uk-gain-pace-amid-trump-tariff-turmoil-and-labour-cost-increases', 'section_name': 'Business', 'category': 'business', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://www.theguardian.com/uk/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/business>\n",
            "{'title': 'Britons increasingly swapping Med’s busy hotspots for ‘destination dupes’', 'url': 'https://www.theguardian.com/business/2025/may/10/britons-swapping-meds-busy-hotspots-destination-dupes', 'section_name': 'Business', 'category': 'business', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://www.theguardian.com/uk/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/business>\n",
            "{'title': 'Ryanair’s £79 membership scheme takes off – but Which? says ‘think twice’', 'url': 'https://www.theguardian.com/business/2025/may/10/ryanair-membership-scheme-which-advises-read-the-small-print', 'section_name': 'Business', 'category': 'business', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://www.theguardian.com/uk/business'}\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://edition.cnn.com/business> (referer: https://edition.cnn.com)\n",
            "INFO:enhanced_news_spider:Extracting articles from section page: https://edition.cnn.com/business (Category: business)\n",
            "2025-05-12 13:12:58 [enhanced_news_spider] INFO: Extracting articles from section page: https://edition.cnn.com/business (Category: business)\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/business>\n",
            "{'title': 'Dow set to soar 1,000 points after Trump team dramatically lowers tariffs with China', 'url': 'https://edition.cnn.com/2025/05/12/investing/stock-market-dow-trade-deal-china', 'section_name': 'Business', 'category': 'business', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://edition.cnn.com/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/business>\n",
            "{'title': 'Despite tariffs, car prices aren’t shooting up. That’s not necessarily a good thing', 'url': 'https://edition.cnn.com/2025/05/12/business/car-prices-tariffs-recession-economy', 'section_name': 'Business', 'category': 'business', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://edition.cnn.com/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/business>\n",
            "{'title': 'Trump’s tariffs could boost some ‘Made in America’ small businesses. But for many, they only hurt', 'url': 'https://edition.cnn.com/2025/05/12/economy/american-made-businesses-tariffs', 'section_name': 'Business', 'category': 'business', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://edition.cnn.com/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/business>\n",
            "{'title': 'Brandon Bell/Getty Images', 'url': 'https://edition.cnn.com/2025/05/12/business/oil-trump-trade-war-gas-prices', 'section_name': 'Business', 'category': 'business', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://edition.cnn.com/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/business>\n",
            "{'title': 'Fabrice Coffrini/AFP/Getty Images', 'url': 'https://edition.cnn.com/2025/05/12/business/us-china-trade-deal-announcement-intl-hnk', 'section_name': 'Business', 'category': 'business', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://edition.cnn.com/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/business>\n",
            "{'title': 'Valentin Flauraud/AFP/Getty Images', 'url': 'https://edition.cnn.com/2025/05/11/business/stock-futures-china-tariff-deal', 'section_name': 'Business', 'category': 'business', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://edition.cnn.com/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/business>\n",
            "{'title': 'Tom Mooney/The Providence Journal/USA Today Network/Imagn Images', 'url': 'https://edition.cnn.com/2025/05/10/business/thc-drinks-legal-farm-bill', 'section_name': 'Business', 'category': 'business', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://edition.cnn.com/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/business>\n",
            "{'title': 'Mert Alper Dervis/Anadolu/Getty Images', 'url': 'https://edition.cnn.com/2025/05/10/business/mothers-day-flowers-tariffs', 'section_name': 'Business', 'category': 'business', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://edition.cnn.com/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/business>\n",
            "{'title': 'David Swanson/AFP/Getty Images', 'url': 'https://edition.cnn.com/2025/05/11/tech/google-facebook-silicon-valley-changes', 'section_name': 'Business', 'category': 'business', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://edition.cnn.com/business'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/business>\n",
            "{'title': 'Oldest toy store in Los Angeles fights to survive in face of tariffs', 'url': 'https://edition.cnn.com/2025/05/11/business/toys-tariffs-los-angeles-store', 'section_name': 'Business', 'category': 'business', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:58', 'section_url': 'https://edition.cnn.com/business'}\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://edition.cnn.com/sport> (referer: https://edition.cnn.com)\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.theguardian.com/uk/culture> (referer: https://www.theguardian.com/international)\n",
            "INFO:enhanced_news_spider:Extracting articles from section page: https://edition.cnn.com/sport (Category: sports)\n",
            "2025-05-12 13:12:59 [enhanced_news_spider] INFO: Extracting articles from section page: https://edition.cnn.com/sport (Category: sports)\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/sport>\n",
            "{'title': 'Runaway goat attempts to ram cyclist off bike in freak incident during Giro d’Italia', 'url': 'https://edition.cnn.com/2025/05/12/sport/giro-ditalia-goat-collision-spt-intl', 'section_name': 'Sports', 'category': 'sports', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:59', 'section_url': 'https://edition.cnn.com/sport'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/sport>\n",
            "{'title': 'Trevor Ruszkowski/Imagn Images/Reuters', 'url': 'https://edition.cnn.com/2025/05/12/sport/indiana-pacers-cleveland-cavaliers-game-4-nba-spt', 'section_name': 'Sports', 'category': 'sports', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:59', 'section_url': 'https://edition.cnn.com/sport'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/sport>\n",
            "{'title': 'Brynn Anderson/AP', 'url': 'https://edition.cnn.com/2025/05/12/sport/junior-alvarado-punishment-sovereignty-kentucky-derby-spt', 'section_name': 'Sports', 'category': 'sports', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:59', 'section_url': 'https://edition.cnn.com/sport'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/sport>\n",
            "{'title': 'David Gray/AFP/Getty Images', 'url': 'https://edition.cnn.com/2025/05/12/sport/virat-kohli-retire-test-cricket-india-spt-intl', 'section_name': 'Sports', 'category': 'sports', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:59', 'section_url': 'https://edition.cnn.com/sport'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/sport>\n",
            "{'title': 'Matthew Stockman/Getty Images', 'url': 'https://edition.cnn.com/2025/05/11/sport/colorado-rockies-bud-black-padres-loss-spt', 'section_name': 'Sports', 'category': 'sports', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:59', 'section_url': 'https://edition.cnn.com/sport'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/sport>\n",
            "{'title': 'Sue Ogrocki/AP', 'url': 'https://edition.cnn.com/2025/05/11/sport/shedeur-sanders-rookies-nfl-practice-spt', 'section_name': 'Sports', 'category': 'sports', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:59', 'section_url': 'https://edition.cnn.com/sport'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/sport>\n",
            "{'title': 'Nipah Dennis/AFP via Getty Images', 'url': 'https://edition.cnn.com/sport/africa-arm-wrestling-olympics-asibey-spc-intl', 'section_name': 'Sports', 'category': 'sports', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:59', 'section_url': 'https://edition.cnn.com/sport'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/sport>\n",
            "{'title': 'American Danielle Collins stuns world No. 2 Iga Świątek, ending her reign at Italian Open', 'url': 'https://edition.cnn.com/2025/05/11/sport/jannik-sinner-return-italian-open-doping-ban-spt-intl', 'section_name': 'Sports', 'category': 'sports', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:59', 'section_url': 'https://edition.cnn.com/sport'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/sport>\n",
            "{'title': 'Trendlines: NBA playoff ticket prices are out of control', 'url': 'https://edition.cnn.com/2025/05/10/sport/trendlines-nba-playoff-ticket-prices-spt', 'section_name': 'Sports', 'category': 'sports', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:59', 'section_url': 'https://edition.cnn.com/sport'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://edition.cnn.com/sport>\n",
            "{'title': 'Nikola Jokić arrives wearing Joker-inspired suit, helps Denver Nuggets to overtime playoff win over Oklahoma City Thunder', 'url': 'https://edition.cnn.com/2025/05/10/sport/nikola-jokic-joker-nuggets-thunder-nba-playoffs-spt-intl', 'section_name': 'Sports', 'category': 'sports', 'source': 'CNN', 'timestamp': '2025-05-12 13:12:59', 'section_url': 'https://edition.cnn.com/sport'}\n",
            "INFO:enhanced_news_spider:Extracting articles from section page: https://www.theguardian.com/uk/culture (Category: culture)\n",
            "2025-05-12 13:13:00 [enhanced_news_spider] INFO: Extracting articles from section page: https://www.theguardian.com/uk/culture (Category: culture)\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/culture>\n",
            "{'title': '‘Danny Dyer deserved nothing!’ The biggest mistakes from the 2025 TV Bafta awards', 'url': 'https://www.theguardian.com/tv-and-radio/2025/may/11/danny-dyer-deserved-nothing-the-biggest-mistakes-from-the-2025-tv-bafta-awards', 'section_name': 'Culture', 'category': 'culture', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:13:00', 'section_url': 'https://www.theguardian.com/uk/culture'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/culture>\n",
            "{'title': 'Bafta TV awards 2025: the full list of winners', 'url': 'https://www.theguardian.com/tv-and-radio/2025/may/11/bafta-tv-awards-2025-the-full-list-of-winners-live', 'section_name': 'Culture', 'category': 'culture', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:13:00', 'section_url': 'https://www.theguardian.com/uk/culture'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/culture>\n",
            "{'title': '‘It spoke to the nation’ – Mr Bates vs the Post Office wins big at Bafta TV awards', 'url': 'https://www.theguardian.com/tv-and-radio/2025/may/11/itv-mr-bates-vs-the-post-office-stars-at-bafta-tv-awards', 'section_name': 'Culture', 'category': 'culture', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:13:00', 'section_url': 'https://www.theguardian.com/uk/culture'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/culture>\n",
            "{'title': 'Cuddles and tears backstage at the 2025 TV Baftas – in pictures', 'url': 'https://www.theguardian.com/tv-and-radio/gallery/2025/may/12/cuddles-and-tears-backstage-at-the-2025-tv-baftas-in-pictures', 'section_name': 'Culture', 'category': 'culture', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:13:00', 'section_url': 'https://www.theguardian.com/uk/culture'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/culture>\n",
            "{'title': 'The one change that worked: I started sketching – and stopped doomscrolling', 'url': 'https://www.theguardian.com/artanddesign/2025/may/12/the-one-change-that-worked-i-started-sketching-and-stopped-doomscrolling', 'section_name': 'Culture', 'category': 'culture', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:13:00', 'section_url': 'https://www.theguardian.com/uk/culture'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/culture>\n",
            "{'title': '‘LSD? Been there, done that’: the Grateful Dead’s 60 years of drugs, epic noodling and obsessive fans', 'url': 'https://www.theguardian.com/music/2025/may/12/lsd-grateful-deads-bobby-weir-jerry-garcia-swifties', 'section_name': 'Culture', 'category': 'culture', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:13:00', 'section_url': 'https://www.theguardian.com/uk/culture'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/culture>\n",
            "{'title': '‘A plea for tolerance’: why Wagon Master is my feelgood movie', 'url': 'https://www.theguardian.com/film/2025/may/12/wagon-master-john-ford-feelgood-movie', 'section_name': 'Culture', 'category': 'culture', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:13:00', 'section_url': 'https://www.theguardian.com/uk/culture'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/culture>\n",
            "{'title': '‘I add the human touch’: the beautiful, bespoke work of Berlin’s last cinema poster artist', 'url': 'https://www.theguardian.com/artanddesign/2025/may/12/i-add-the-human-touch-the-beautiful-bespoke-work-of-berlins-last-cinema-poster-artist', 'section_name': 'Culture', 'category': 'culture', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:13:00', 'section_url': 'https://www.theguardian.com/uk/culture'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/culture>\n",
            "{'title': 'Bibaa & Nicole: Murder in the Park review – the bigotry of the police is still barely believable', 'url': 'https://www.theguardian.com/tv-and-radio/2025/may/11/bibaa-and-nicole-murder-in-the-park-review-sky-documentaries-now-tv', 'section_name': 'Culture', 'category': 'culture', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:13:00', 'section_url': 'https://www.theguardian.com/uk/culture'}\n",
            "DEBUG:scrapy.core.scraper:Scraped from <200 https://www.theguardian.com/uk/culture>\n",
            "{'title': '‘A whirling mass of limbs and lingerie’: the salacious, riotous story of the high-kicking cancan', 'url': 'https://www.theguardian.com/stage/2025/may/12/limbs-lingerie-salacious-riotous-high-kicking-cancan-bottoms-dance', 'section_name': 'Culture', 'category': 'culture', 'source': 'The Guardian', 'timestamp': '2025-05-12 13:13:00', 'section_url': 'https://www.theguardian.com/uk/culture'}\n",
            "INFO:scrapy.core.engine:Closing spider (finished)\n",
            "2025-05-12 13:13:00 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "INFO:scrapy.extensions.feedexport:Stored csv feed (140 items) in: enhanced_news_articles.csv\n",
            "2025-05-12 13:13:00 [scrapy.extensions.feedexport] INFO: Stored csv feed (140 items) in: enhanced_news_articles.csv\n",
            "INFO:scrapy.statscollectors:Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 9425,\n",
            " 'downloader/request_count': 22,\n",
            " 'downloader/request_method_count/GET': 22,\n",
            " 'downloader/response_bytes': 4038315,\n",
            " 'downloader/response_count': 22,\n",
            " 'downloader/response_status_count/200': 18,\n",
            " 'downloader/response_status_count/301': 1,\n",
            " 'downloader/response_status_count/302': 2,\n",
            " 'downloader/response_status_count/404': 1,\n",
            " 'elapsed_time_seconds': 7.235496,\n",
            " 'feedexport/success_count/FileFeedStorage': 1,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2025, 5, 12, 13, 13, 0, 93689, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 22811550,\n",
            " 'httpcompression/response_count': 19,\n",
            " 'httperror/response_ignored_count': 1,\n",
            " 'httperror/response_ignored_status_count/404': 1,\n",
            " 'item_scraped_count': 140,\n",
            " 'items_per_minute': 1200.0,\n",
            " 'log_count/INFO': 48,\n",
            " 'memusage/max': 149123072,\n",
            " 'memusage/startup': 149123072,\n",
            " 'request_depth_max': 1,\n",
            " 'response_received_count': 19,\n",
            " 'responses_per_minute': 162.85714285714286,\n",
            " 'scheduler/dequeued': 22,\n",
            " 'scheduler/dequeued/memory': 22,\n",
            " 'scheduler/enqueued': 22,\n",
            " 'scheduler/enqueued/memory': 22,\n",
            " 'start_time': datetime.datetime(2025, 5, 12, 13, 12, 52, 858193, tzinfo=datetime.timezone.utc)}\n",
            "2025-05-12 13:13:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 9425,\n",
            " 'downloader/request_count': 22,\n",
            " 'downloader/request_method_count/GET': 22,\n",
            " 'downloader/response_bytes': 4038315,\n",
            " 'downloader/response_count': 22,\n",
            " 'downloader/response_status_count/200': 18,\n",
            " 'downloader/response_status_count/301': 1,\n",
            " 'downloader/response_status_count/302': 2,\n",
            " 'downloader/response_status_count/404': 1,\n",
            " 'elapsed_time_seconds': 7.235496,\n",
            " 'feedexport/success_count/FileFeedStorage': 1,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2025, 5, 12, 13, 13, 0, 93689, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 22811550,\n",
            " 'httpcompression/response_count': 19,\n",
            " 'httperror/response_ignored_count': 1,\n",
            " 'httperror/response_ignored_status_count/404': 1,\n",
            " 'item_scraped_count': 140,\n",
            " 'items_per_minute': 1200.0,\n",
            " 'log_count/INFO': 48,\n",
            " 'memusage/max': 149123072,\n",
            " 'memusage/startup': 149123072,\n",
            " 'request_depth_max': 1,\n",
            " 'response_received_count': 19,\n",
            " 'responses_per_minute': 162.85714285714286,\n",
            " 'scheduler/dequeued': 22,\n",
            " 'scheduler/dequeued/memory': 22,\n",
            " 'scheduler/enqueued': 22,\n",
            " 'scheduler/enqueued/memory': 22,\n",
            " 'start_time': datetime.datetime(2025, 5, 12, 13, 12, 52, 858193, tzinfo=datetime.timezone.utc)}\n",
            "INFO:scrapy.core.engine:Spider closed (finished)\n",
            "2025-05-12 13:13:00 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scrapy spider (NewsSpider) finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: convert the above csv into excel\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "try:\n",
        "    df = pd.read_csv('enhanced_news_articles.csv')\n",
        "\n",
        "    # Convert the DataFrame to an Excel file\n",
        "    df.to_excel('enhanced_news_articles.xlsx', index=False)  # Set index=False to avoid writing row indices\n",
        "\n",
        "    print(\"CSV file successfully converted to Excel.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'enhanced_news_articles.csv' not found. Please make sure the file exists.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwquDP9roLbN",
        "outputId": "6c4ce3da-af97-4ee2-deb7-8a6ab590bb08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "2025-05-12 13:20:21 [numexpr.utils] INFO: NumExpr defaulting to 2 threads.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file successfully converted to Excel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe09iE_rVZUU",
        "outputId": "a461651b-5154-41ef-91e0-6a5393e165ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully converted 'enhanced_news_articles.csv' to 'enhanced_news_articles.xlsx'\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8kQ_MPaVrem",
        "outputId": "b8c4491c-99ff-4a2f-86b2-9371a07fd1d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Clustering Process ---\n",
            "Processing 888 articles for clustering...\n",
            "Performing K-Means with 5 clusters...\n",
            "Silhouette Score: 0.08\n",
            "Model saved to kmeans_model.pkl, Vectorizer to tfidf_vectorizer.pkl\n",
            "Clustered articles saved to 'clustered_news_articles.csv'\n",
            "\n",
            "Cluster Distribution:\n",
            " cluster_label\n",
            "0     35\n",
            "1     14\n",
            "2     13\n",
            "3    749\n",
            "4     77\n",
            "Name: count, dtype: int64\n",
            "--- Clustering Process Finished ---\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olFTR7IlVxZG",
        "outputId": "8800e2a2-fcc6-468c-8ed9-138edff40ac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask templates created in 'templates/' directory.\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOrO1133YWeJ",
        "outputId": "80a7a73c-f981-400d-c257-e1da36861e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.8-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.8\n"
          ]
        }
      ],
      "source": [
        "pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iRmLPBqoV0-h",
        "outputId": "42ba9fb0-2ee1-4d5e-cf6d-03cd43071dbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask App Cell: NGROK_AUTHTOKEN from env: None\n",
            "CRITICAL: NGROK_AUTHTOKEN was not found in the environment. ngrok will likely fail.\n",
            "Ngrok authtoken not set. Flask app will run locally only.\n",
            "Starting Flask app...\n",
            "Flask app will run locally on http://127.0.0.1:5000 but (potentially) NOT via ngrok.\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "2025-05-12 13:27:39 [werkzeug] INFO: \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "2025-05-12 13:27:39 [werkzeug] INFO: \u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqTt6PQrV5Qj",
        "outputId": "52f90a33-b308-40c2-a27c-9e44aaddf159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask App Cell: NGROK_AUTHTOKEN from env: None\n",
            "CRITICAL: NGROK_AUTHTOKEN was not found in the environment. ngrok will likely fail.\n",
            "Starting Flask app with ngrok...\n",
            "Ensure 'clustered_news_articles.csv' exists (created by Cell 5).\n",
            "Your public URL from ngrok will appear shortly below.\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "2025-05-12 13:28:05 [werkzeug] INFO: \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "2025-05-12 13:28:05 [werkzeug] INFO: \u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): bin.equinox.io:443\n",
            "DEBUG:urllib3.connectionpool:https://bin.equinox.io:443 \"GET /c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip HTTP/1.1\" 200 13921656\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4040\n",
            "Exception in thread Thread-8:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\", line 198, in _new_conn\n",
            "    sock = connection.create_connection(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
            "    sock.connect(sa)\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
            "    response = self._make_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
            "    conn.request(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\", line 445, in request\n",
            "    self.endheaders()\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 1298, in endheaders\n",
            "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 1058, in _send_output\n",
            "    self.send(msg)\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 996, in send\n",
            "    self.connect()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\", line 276, in connect\n",
            "    self.sock = self._new_conn()\n",
            "                ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\", line 213, in _new_conn\n",
            "    raise NewConnectionError(\n",
            "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7e7df0b67750>: Failed to establish a new connection: [Errno 111] Connection refused\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 667, in send\n",
            "    resp = conn.urlopen(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
            "    retries = retries.increment(\n",
            "              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/util/retry.py\", line 519, in increment\n",
            "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e7df0b67750>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1401, in run\n",
            "    self.function(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/flask_ngrok.py\", line 70, in start_ngrok\n",
            "    ngrok_address = _run_ngrok()\n",
            "                    ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/flask_ngrok.py\", line 35, in _run_ngrok\n",
            "    tunnel_url = requests.get(localhost_url).text  # Get the tunnel information\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/api.py\", line 73, in get\n",
            "    return request(\"get\", url, params=params, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/api.py\", line 59, in request\n",
            "    return session.request(method=method, url=url, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 700, in send\n",
            "    raise ConnectionError(e, request=request)\n",
            "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e7df0b67750>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqDY_JPpWrSG"
      },
      "outputs": [],
      "source": [
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from scrapy.linkextractors import LinkExtractor\n",
        "from datetime import datetime\n",
        "import re\n",
        "import logging\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "class NewsSpider(scrapy.Spider):\n",
        "    name = 'enhanced_news_spider'\n",
        "    # Define the domains this spider is allowed to crawl\n",
        "    allowed_domains = [\n",
        "        'bbc.com',\n",
        "        'bbc.co.uk', # Important for BBC\n",
        "        'cnn.com',\n",
        "        'aljazeera.com',\n",
        "        'theguardian.com'\n",
        "        ]\n",
        "    custom_settings = {\n",
        "        # ... (rest of your custom_settings)\n",
        "        'FEED_URI': 'enhanced_news_articles_filtered.csv', # Changed output name\n",
        "        # ...\n",
        "    }\n",
        "\n",
        "    # Keep the SECTION_MAPPING, ALTERNATIVE_CHANNELS, start_urls as before\n",
        "    SECTION_MAPPING = {\n",
        "        'business': ['business', 'economy', 'finance', 'markets', 'money', 'invest', 'stocks', 'companies', 'corporate', 'trade'],\n",
        "        'politics': ['politics', 'government', 'election', 'parliament', 'congress', 'democracy'],\n",
        "        'sports': ['sports', 'football', 'soccer', 'basketball', 'tennis', 'golf', 'olympics'],\n",
        "        'culture': ['culture', 'arts', 'entertainment', 'movies', 'music', 'television', 'theatre', 'gallery', 'celebrities']\n",
        "    }\n",
        "\n",
        "    ALTERNATIVE_CHANNELS = {\n",
        "        'business': ['https://www.bbc.com/news/business', 'https://edition.cnn.com/business', 'https://www.aljazeera.com/economy/'],\n",
        "        'politics': ['https://www.aljazeera.com/politics/', 'https://www.theguardian.com/politics'],\n",
        "        'sports': ['https://www.bbc.com/sport', 'https://edition.cnn.com/sport'],\n",
        "        'culture': ['https://www.bbc.com/culture', 'https://www.theguardian.com/uk/culture']\n",
        "    }\n",
        "\n",
        "    start_urls = [\n",
        "        'https://www.bbc.com',\n",
        "        'https://edition.cnn.com',\n",
        "        'https://www.aljazeera.com',\n",
        "        'https://www.theguardian.com/international',\n",
        "    ]\n",
        "\n",
        "\n",
        "    # __init__, parse, discover_sections remain largely the same\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(NewsSpider, self).__init__(*args, **kwargs)\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Check if the response URL is actually from an allowed domain\n",
        "        # This adds an extra layer of safety, though allowed_domains should handle it\n",
        "        is_allowed = any(domain in response.url for domain in self.allowed_domains)\n",
        "        if not is_allowed:\n",
        "             self.logger.debug(f\"Ignoring response from disallowed domain: {response.url}\")\n",
        "             return # Stop processing this response\n",
        "\n",
        "        # --- Rest of your parse logic ---\n",
        "        if response.meta.get('is_section_page'):\n",
        "            self.logger.info(f\"Extracting articles from section page: {response.url} (Category: {response.meta.get('category')})\")\n",
        "            yield from self.extract_articles(response)\n",
        "        elif response.url in self.start_urls:\n",
        "            self.logger.info(f\"Discovering sections from homepage: {response.url}\")\n",
        "            yield from self.discover_sections(response)\n",
        "        else:\n",
        "            # This case might still happen if a section URL redirects, check domain again\n",
        "            self.logger.warning(f\"Unexpected URL in parse() from allowed domain: {response.url}. Attempting article extraction.\")\n",
        "            if 'source' not in response.meta: response.meta['source'] = self.get_source_name(response.url)\n",
        "            if 'category' not in response.meta: response.meta['category'] = self.detect_category(response)\n",
        "            if 'potential_section_name' not in response.meta: response.meta['potential_section_name'] = response.meta.get('category', 'Unknown Section').capitalize()\n",
        "            yield from self.extract_articles(response)\n",
        "\n",
        "\n",
        "    def discover_sections(self, response):\n",
        "        # LinkExtractor respects allowed_domains by default\n",
        "        link_extractor = LinkExtractor(\n",
        "            restrict_css='nav, ul.menu, div.navigation, header nav, footer nav, .primary-nav, #orb-nav-links',\n",
        "            deny_extensions=['jpg', 'png', 'pdf', 'mp4', 'mp3', 'zip', 'gz', 'css', 'js'],\n",
        "            unique=True,\n",
        "            # Explicitly adding allowed_domains here is redundant but safe\n",
        "            allow_domains=self.allowed_domains\n",
        "        )\n",
        "        # --- Rest of discover_sections logic ---\n",
        "        found_categories_on_page = set()\n",
        "        current_source_main_domain = '/'.join(response.url.split('/')[:3]) # Get scheme + domain\n",
        "\n",
        "        for link in link_extractor.extract_links(response):\n",
        "             url_lower = link.url.lower()\n",
        "             text_lower = link.text.lower().strip() if link.text else ''\n",
        "             if not text_lower and '#' in link.url: continue\n",
        "\n",
        "             # Ensure the extracted link's domain is allowed (LinkExtractor should do this, but belt-and-suspenders)\n",
        "             link_domain_allowed = any(domain in link.url for domain in self.allowed_domains)\n",
        "             if not link_domain_allowed:\n",
        "                 # self.logger.debug(f\"LinkExtractor returned disallowed link (shouldn't happen often): {link.url}\")\n",
        "                 continue\n",
        "\n",
        "             for category, keywords in self.SECTION_MAPPING.items():\n",
        "                 if any(kw in url_lower or kw in text_lower for kw in keywords):\n",
        "                     if category not in found_categories_on_page:\n",
        "                         self.logger.info(f\"Found potential section link for '{category}': '{link.text}' ({link.url}) on {response.url}\")\n",
        "                         found_categories_on_page.add(category)\n",
        "                         yield scrapy.Request(\n",
        "                             url=response.urljoin(link.url),\n",
        "                             callback=self.parse,\n",
        "                             meta={\n",
        "                                 'category': category,\n",
        "                                 'source': self.get_source_name(link.url), # Get source from the link URL\n",
        "                                 'potential_section_name': link.text.strip() if link.text else category.capitalize(),\n",
        "                                 'is_section_page': True\n",
        "                             }\n",
        "                         )\n",
        "                         break # Move to the next link\n",
        "\n",
        "        missing_categories_for_this_source = set(self.SECTION_MAPPING.keys()) - found_categories_on_page\n",
        "        current_hostname = response.url.split('/')[2] # e.g., www.bbc.com\n",
        "        for category in missing_categories_for_this_source:\n",
        "             # self.logger.info(f\"Checking ALTERNATIVE_CHANNELS for '{category}' on {current_hostname}.\")\n",
        "             for alt_url in self.ALTERNATIVE_CHANNELS.get(category, []):\n",
        "                 # Check if the alternative URL belongs to the *same* base domain being processed\n",
        "                 alt_hostname = alt_url.split('/')[2]\n",
        "                 is_same_source = any(domain in current_hostname and domain in alt_hostname for domain in self.allowed_domains if '.' in domain) # Check base domain match\n",
        "\n",
        "                 if is_same_source:\n",
        "                     self.logger.info(f\"Attempting alternative URL for '{category}' from {current_hostname}: {alt_url}\")\n",
        "                     yield scrapy.Request(\n",
        "                         url=alt_url, callback=self.parse,\n",
        "                         meta={'category': category, 'source': self.get_source_name(alt_url),\n",
        "                               'potential_section_name': category.capitalize(), 'is_section_page': True}\n",
        "                     )\n",
        "                     break # Found an alt URL for this source, move to next category\n",
        "\n",
        "\n",
        "    def extract_articles(self, response):\n",
        "        # --- Get metadata as before ---\n",
        "        source = response.meta.get('source', self.get_source_name(response.url))\n",
        "        category = response.meta.get('category', self.detect_category(response))\n",
        "        potential_section_name = response.meta.get('potential_section_name', category.capitalize())\n",
        "        section_name = self.extract_section_name(response, potential_section_name)\n",
        "        if not section_name: section_name = category.capitalize()\n",
        "\n",
        "        # --- Article selectors map etc. as before ---\n",
        "        article_selectors_map = {\n",
        "            'bbc.com': ['a[type=\"article\"]', 'a[href*=\"/news/articles/\"]', '.gs-c-promo-heading[class*=\"__title\"] a'],\n",
        "            'cnn.com': ['a[data-link-type=\"article\"]', '.card a', 'a[href*=\"/videos/\"]'],\n",
        "            'aljazeera.com': ['a.u-clickable-card__link', 'article h3 a', 'a.article-trending__title-link'],\n",
        "            'theguardian.com': ['a[data-link-name=\"article\"]', '.fc-item__link', 'a[aria-label*=\"article\"]']\n",
        "        }\n",
        "        selector_list_for_domain = []\n",
        "        for domain_key in article_selectors_map:\n",
        "            if domain_key in response.url:\n",
        "                selector_list_for_domain = article_selectors_map[domain_key]; break\n",
        "        generic_selectors = ['article a[href]', 'div[class*=\"article\"] a[href]', 'div[class*=\"post\"] a[href]', 'div[class*=\"item\"] a[href]', 'a[href*=\"/article/\"]', 'a[href*=\"/story/\"]', 'a[href*=\"/news/\"]','h2 a[href]', 'h3 a[href]']\n",
        "        combined_selectors = selector_list_for_domain + generic_selectors\n",
        "\n",
        "        articles_found_on_page = set(); article_count = 0\n",
        "        MAX_ARTICLES_PER_SECTION = 10\n",
        "\n",
        "        for selector_str in combined_selectors:\n",
        "            if article_count >= MAX_ARTICLES_PER_SECTION: break\n",
        "            for article_element in response.css(selector_str):\n",
        "                if article_count >= MAX_ARTICLES_PER_SECTION: break\n",
        "\n",
        "                href = article_element.css('::attr(href)').get()\n",
        "                if not href or href.startswith('#') or href.startswith('javascript:'): continue\n",
        "\n",
        "                full_url = response.urljoin(href)\n",
        "\n",
        "                # **** Crucial Check: Ensure the extracted URL is within allowed domains ****\n",
        "                if not any(domain in full_url for domain in self.allowed_domains):\n",
        "                    # self.logger.debug(f\"Skipping link to disallowed domain: {full_url}\")\n",
        "                    continue\n",
        "                # **** End Crucial Check ****\n",
        "\n",
        "                if full_url in articles_found_on_page or not (full_url.startswith('http://') or full_url.startswith('https://')): continue\n",
        "                # ... (rest of the filtering like domain root, mailto, etc.)\n",
        "\n",
        "                # --- Title extraction and validation as before ---\n",
        "                title_parts = article_element.xpath(\".//text()[normalize-space()]\").getall()\n",
        "                title = \" \".join(part.strip() for part in title_parts if part.strip()).strip()\n",
        "                title = re.sub(r'\\s+', ' ', title).strip()\n",
        "\n",
        "                if not title or len(title) < 10:\n",
        "                    potential_titles = [\n",
        "                        article_element.css('h1::text, h2::text, h3::text, h4::text').get(),\n",
        "                        article_element.css('span[class*=\"title\"]::text').get(),\n",
        "                        article_element.css('::attr(title)').get(),\n",
        "                        article_element.css('::attr(aria-label)').get(),\n",
        "                        article_element.xpath('string(.)').get()\n",
        "                    ]\n",
        "                    for pt in potential_titles:\n",
        "                         if pt and len(pt.strip()) >= 10:\n",
        "                             title = re.sub(r'\\s+', ' ', pt.strip()).strip()\n",
        "                             break\n",
        "\n",
        "                if not title or len(title) < 10 or len(title) > 200: continue\n",
        "\n",
        "                # --- Optional: Cross-section link check as before ---\n",
        "                url_lower_path = '/'.join(full_url.lower().split('/')[3:])\n",
        "                different_category_found = False\n",
        "                for cat, kws in self.SECTION_MAPPING.items():\n",
        "                     if cat != category and any(kw in url_lower_path for kw in kws):\n",
        "                         different_category_found = True; break\n",
        "                if different_category_found: continue\n",
        "\n",
        "                # --- Yield item ---\n",
        "                articles_found_on_page.add(full_url)\n",
        "                # **** Re-confirm source name based on the *final* article URL ****\n",
        "                final_source = self.get_source_name(full_url)\n",
        "                if final_source == 'Unknown Source':\n",
        "                    self.logger.warning(f\"Could not determine source for allowed URL: {full_url}. Falling back to response source: {source}\")\n",
        "                    final_source = source # Use the source from the page it was found on as fallback\n",
        "\n",
        "                yield {\n",
        "                    'title': title,\n",
        "                    'url': full_url,\n",
        "                    'section_name': section_name,\n",
        "                    'category': category,\n",
        "                    'source': final_source, # Use source derived from the article URL\n",
        "                    'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                    'section_url': response.url\n",
        "                    }\n",
        "                article_count += 1\n",
        "\n",
        "        if article_count == 0:\n",
        "            self.logger.info(f\"No valid articles extracted from allowed domain page {response.url} (Category: {category}, Section: {section_name})\")\n",
        "\n",
        "\n",
        "    # extract_section_name remains the same\n",
        "\n",
        "    def extract_section_name(self, response, fallback_name):\n",
        "        selectors = ['h1.section-header__title::text', 'h1[class*=\"PageTitle\"]::text', 'h1[class*=\"section-title\"]::text', '.page-title ::text', 'h1[itemprop=\"name\"]::text', 'header h1::text', 'h1::text']\n",
        "        section_name_str = ''\n",
        "        for sel in selectors:\n",
        "            extracted_name = response.css(sel).get()\n",
        "            if extracted_name and extracted_name.strip():\n",
        "                section_name_str = extracted_name.strip(); break\n",
        "        if not section_name_str: section_name_str = fallback_name\n",
        "        section_name_str = re.sub(r'[^\\w\\s-]', '', section_name_str).strip()\n",
        "        section_name_str = re.sub(r'\\s+', ' ', section_name_str)\n",
        "        if not section_name_str:\n",
        "            try:\n",
        "                path_parts = [p for p in response.url.split('/') if p]\n",
        "                generic = {'www', 'com', 'co', 'uk', 'org', 'net', 'html', 'htm', 'php', 'asp', 'news', 'article', 'articles', 'category', 'categories', 'section', 'sections', 'world', 'international', 'us', 'uk', 'europe', 'asia', 'africa'}\n",
        "                for part in reversed(path_parts[-3:]):\n",
        "                    cleaned = part.split('.')[0].lower()\n",
        "                    if cleaned and len(cleaned) > 3 and cleaned not in generic and not cleaned.isdigit():\n",
        "                        section_name_str = cleaned.replace('-', ' ').replace('_', ' ').title(); break\n",
        "            except Exception: pass\n",
        "        return section_name_str[:100] if section_name_str else fallback_name[:100]\n",
        "\n",
        "\n",
        "    # get_source_name remains the same - it's okay for it to return 'Unknown Source'\n",
        "    # if allowed_domains filter works correctly. We added a fallback in extract_articles.\n",
        "    def get_source_name(self, url):\n",
        "        domain_map = {'bbc.com': 'BBC', 'bbc.co.uk': 'BBC', 'cnn.com': 'CNN', 'aljazeera.com': 'Al Jazeera', 'theguardian.com': 'The Guardian'}\n",
        "        for domain, name in domain_map.items():\n",
        "            if domain in url: return name\n",
        "        try:\n",
        "            hostname = url.split('//')[-1].split('/')[0]\n",
        "            parts = hostname.replace('www.', '').split('.')\n",
        "            if len(parts) > 2 and len(parts[-2]) <= 3:\n",
        "                 source_guess = parts[-3]\n",
        "            else:\n",
        "                 source_guess = parts[0]\n",
        "            return source_guess.capitalize()\n",
        "        except Exception:\n",
        "            # Log the URL that caused the issue for debugging\n",
        "            self.logger.error(f\"Failed to parse domain for URL: {url}\")\n",
        "            return 'Unknown Source' # Fallback\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\": section remains the same\n",
        "if __name__ == \"__main__\":\n",
        "    output_file = NewsSpider.custom_settings['FEED_URI']\n",
        "    print(f\"Starting Scrapy spider (NewsSpider)... Output: {output_file}\")\n",
        "    process = CrawlerProcess(settings={\n",
        "        'LOG_LEVEL': 'INFO',\n",
        "        'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',\n",
        "        # Ensure FEED_URI from custom_settings is used if not overridden\n",
        "        'FEED_URI': output_file,\n",
        "        'FEED_FORMAT': NewsSpider.custom_settings['FEED_FORMAT'],\n",
        "         # You might want to ensure overwrite is explicitly true here too\n",
        "        'FEED_OVERWRITE': True,\n",
        "    })\n",
        "    process.crawl(NewsSpider)\n",
        "    process.start()\n",
        "    print(f\"Scrapy spider (NewsSpider) finished. Output saved to {output_file}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}